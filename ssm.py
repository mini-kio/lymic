import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional
from torch.utils.checkpoint import checkpoint

class OptimizedS6Block(nn.Module):
    """
    ğŸš€ ìµœì í™”ëœ S6 (Selective State Space) Block
    - FP16 ìµœì í™”
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  êµ¬í˜„
    - ì»´íŒŒì¼ ìµœì í™” ì§€ì›
    - ë” ë¹ ë¥¸ ì„ íƒì  ìŠ¤ìº”
    """
    def __init__(self, d_model, d_state=64, expand_factor=2, use_fast_conv=True):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_inner = expand_factor * d_model
        self.use_fast_conv = use_fast_conv
        
        # ğŸ”¥ íš¨ìœ¨ì ì¸ ì…ë ¥ í”„ë¡œì ì…˜
        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)
        
        # ğŸ”¥ ì„ íƒì  íŒŒë¼ë¯¸í„° (ìµœì í™”ë¨)
        self.x_proj = nn.Linear(self.d_inner, d_state * 2 + self.d_inner, bias=False)
        self.dt_proj = nn.Linear(d_state, self.d_inner, bias=True)
        
        # ğŸ”¥ ìƒíƒœ ê³µê°„ íŒŒë¼ë¯¸í„° (ì´ˆê¸°í™” ìµœì í™”)
        self.A_log = nn.Parameter(torch.randn(self.d_inner, d_state))
        self.D = nn.Parameter(torch.randn(self.d_inner))
        
        # ğŸ”¥ ì¶œë ¥ í”„ë¡œì ì…˜ (bias ì œê±°ë¡œ ìµœì í™”)
        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)
        
        # ì •ê·œí™”
        self.norm = nn.LayerNorm(d_model)
        
        # ğŸš€ ìµœì í™”ëœ ì´ˆê¸°í™”
        self._optimized_init()
        
        # ì»´íŒŒì¼ ì¤€ë¹„
        self._compiled = False
    
    def _optimized_init(self):
        """ğŸ”¥ ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”"""
        # A í–‰ë ¬: ì•ˆì •ì ì¸ ì´ˆê¸°í™”
        with torch.no_grad():
            # S4/S6ì— ìµœì í™”ëœ ì´ˆê¸°í™”
            nn.init.uniform_(self.A_log, -4.0, -1.0)
            
            # D: ì‘ì€ ì–‘ìˆ˜ ê°’
            nn.init.normal_(self.D, mean=1.0, std=0.1)
            
            # í”„ë¡œì ì…˜ ë ˆì´ì–´ë“¤
            nn.init.xavier_uniform_(self.in_proj.weight)
            nn.init.xavier_uniform_(self.x_proj.weight)
            nn.init.xavier_uniform_(self.dt_proj.weight)
            nn.init.xavier_uniform_(self.out_proj.weight)
            
            # dt_proj bias ì´ˆê¸°í™”
            nn.init.uniform_(self.dt_proj.bias, -0.1, 0.1)
    
    def compile_block(self):
        """ë¸”ë¡ ì»´íŒŒì¼"""
        if not self._compiled:
            try:
                self.selective_scan = torch.compile(
                    self.selective_scan, 
                    mode='max-autotune',
                    dynamic=True
                )
                self._compiled = True
                print("ğŸš€ S6Block compiled")
            except Exception as e:
                print(f"âš ï¸ S6Block compilation failed: {e}")
    
    @torch.amp.autocast('cuda')
    def forward(self, x):
        """
        ğŸ”¥ ìµœì í™”ëœ ìˆœì „íŒŒ
        Args:
            x: (B, L, D) ì…ë ¥ ì‹œí€€ìŠ¤
        Returns:
            (B, L, D) ì¶œë ¥ ì‹œí€€ìŠ¤
        """
        B, L, D = x.shape
        residual = x
        
        # ğŸ”¥ ì…ë ¥ í”„ë¡œì ì…˜ (í•œ ë²ˆì—)
        x_and_res = self.in_proj(x)  # (B, L, 2*d_inner)
        x, res = x_and_res.split([self.d_inner, self.d_inner], dim=-1)
        
        # SiLU í™œì„±í™”
        x = F.silu(x)
        
        # ğŸš€ ì„ íƒì  ìŠ¤ìº” (ìµœì í™”ë¨)
        x = self.selective_scan(x)
        
        # ì”ì°¨ ì—°ê²°
        x = x * F.silu(res)
        
        # ì¶œë ¥ í”„ë¡œì ì…˜
        x = self.out_proj(x)
        
        # ì •ê·œí™” + ì”ì°¨
        return self.norm(x + residual)
    
    def selective_scan(self, x):
        """
        ğŸš€ ìµœì í™”ëœ ì„ íƒì  ìŠ¤ìº”
        - ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
        - ìˆ˜ì¹˜ì  ì•ˆì •ì„± í–¥ìƒ
        - FP16 ìµœì í™”
        """
        B, L, d_inner = x.shape
        
        # ğŸ”¥ ì„ íƒì  íŒŒë¼ë¯¸í„° ê³„ì‚°
        x_dbl = self.x_proj(x)  # (B, L, d_state*2 + d_inner)
        
        # ì°¨ì› ë¶„í•  ìˆ˜ì •
        delta_B_sel, C_sel = x_dbl.split([self.d_state * 2, self.d_inner], dim=-1)
        B_sel, delta = delta_B_sel.split([self.d_state, self.d_state], dim=-1)
        
        # ğŸ”¥ ì´ì‚°í™” ë‹¨ê³„ (ìˆ˜ì¹˜ì  ì•ˆì •ì„±)
        delta = F.softplus(self.dt_proj(delta))  # (B, L, d_inner)
        delta = torch.clamp(delta, max=10.0)  # ìˆ˜ì¹˜ì  ì•ˆì •ì„±
        
        # ìƒíƒœ ê³µê°„ í–‰ë ¬
        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)
        
        # ğŸš€ íš¨ìœ¨ì ì¸ ì´ì‚°í™”
        # deltaA: (B, L, d_inner, d_state)
        deltaA = torch.exp(delta.unsqueeze(-1) * A.unsqueeze(0).unsqueeze(0))
        
        # deltaB: (B, L, d_inner, d_state) 
        deltaB = delta.unsqueeze(-1) * B_sel.unsqueeze(-2)
        
        # ğŸš€ ìµœì í™”ëœ ìŠ¤ìº” (ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬)
        chunk_size = min(64, L)  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
        outputs = []
        
        # ì´ˆê¸° ìƒíƒœ
        h = torch.zeros(B, d_inner, self.d_state, device=x.device, dtype=x.dtype)
        
        for i in range(0, L, chunk_size):
            end_idx = min(i + chunk_size, L)
            chunk_len = end_idx - i
            
            # ì²­í¬ ë°ì´í„°
            x_chunk = x[:, i:end_idx]  # (B, chunk_len, d_inner)
            deltaA_chunk = deltaA[:, i:end_idx]  # (B, chunk_len, d_inner, d_state)
            deltaB_chunk = deltaB[:, i:end_idx]  # (B, chunk_len, d_inner, d_state)
            C_chunk = C_sel[:, i:end_idx]  # (B, chunk_len, d_inner)
            
            # ì²­í¬ ë‚´ ìŠ¤ìº”
            chunk_outputs = []
            for j in range(chunk_len):
                # ìƒíƒœ ì—…ë°ì´íŠ¸ - ì°¨ì› ì²´í¬ ì¶”ê°€
                h_next = deltaA_chunk[:, j] * h  # (B, d_inner, d_state)
                
                # deltaBì™€ xì˜ ì°¨ì› ë§ì¶¤
                x_expanded = x_chunk[:, j].unsqueeze(-1)  # (B, d_inner, 1)
                deltaB_j = deltaB_chunk[:, j]  # (B, d_inner, d_state)
                
                h = h_next + deltaB_j * x_expanded  # (B, d_inner, d_state)
                
                # ì¶œë ¥ ê³„ì‚° - ì°¨ì› í™•ì¸
                C_expanded = C_chunk[:, j].unsqueeze(-1)  # (B, d_inner, 1)
                y = torch.sum(C_expanded * h, dim=-1)  # (B, d_inner)
                chunk_outputs.append(y)
            
            outputs.extend(chunk_outputs)
        
        y = torch.stack(outputs, dim=1)  # (B, L, d_inner)
        
        # ğŸ”¥ ìŠ¤í‚µ ì—°ê²° (ë¸Œë¡œë“œìºìŠ¤íŒ… ìµœì í™”)
        y = y + x * self.D.view(1, 1, -1)
        
        return y

class FastS6Block(nn.Module):
    """
    ğŸš€ Ultra-fast S6 Block
    - ê·¼ì‚¬ ìŠ¤ìº”ìœ¼ë¡œ ê·¹ëŒ€ ìµœì í™”
    - ì¶”ë¡  ì „ìš© ìµœì í™”
    """
    def __init__(self, d_model, d_state=32, expand_factor=1.5):  # ë” ì‘ì€ íŒŒë¼ë¯¸í„°
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_inner = int(expand_factor * d_model)
        
        # ì¶•ì†Œëœ ë„¤íŠ¸ì›Œí¬
        self.in_proj = nn.Linear(d_model, self.d_inner, bias=False)
        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)
        
        # ê°„ë‹¨í•œ ì„ íƒì  ë©”ì»¤ë‹ˆì¦˜
        self.gate = nn.Linear(d_model, self.d_inner, bias=False)
        
        self.norm = nn.LayerNorm(d_model)
        
        # ì´ˆê¸°í™”
        nn.init.xavier_uniform_(self.in_proj.weight)
        nn.init.xavier_uniform_(self.out_proj.weight)
        nn.init.xavier_uniform_(self.gate.weight)
    
    def forward(self, x):
        """Ultra-fast forward"""
        residual = x
        
        # ë‹¨ìˆœí™”ëœ ì²˜ë¦¬
        h = self.in_proj(x)
        g = torch.sigmoid(self.gate(x))
        
        # ê°„ë‹¨í•œ ì„ íƒì  í™œì„±í™”
        h = h * g * F.silu(h)
        
        # ì¶œë ¥
        x = self.out_proj(h)
        
        return self.norm(x + residual)

class OptimizedS6SSMEncoder(nn.Module):
    """
    ğŸš€ ìµœì í™”ëœ S6-ê¸°ë°˜ SSM ì¸ì½”ë”
    - ë™ì  ë ˆì´ì–´ ì„ íƒ
    - ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
    - ì»´íŒŒì¼ ìµœì í™”
    """
    def __init__(self, d_model=768, n_layers=3, d_state=64, use_fast_layers=False, 
                 use_gradient_checkpointing=False):
        super().__init__()
        
        self.use_gradient_checkpointing = use_gradient_checkpointing
        
        # ğŸ”¥ ë™ì  ë ˆì´ì–´ êµ¬ì„±
        self.layers = nn.ModuleList()
        
        for i in range(n_layers):
            if use_fast_layers and i >= n_layers - 1:
                # ë§ˆì§€ë§‰ ë ˆì´ì–´ëŠ” ultra-fast
                layer = FastS6Block(d_model, d_state//2)
            else:
                # ì¼ë°˜ ìµœì í™”ëœ ë ˆì´ì–´
                layer = OptimizedS6Block(d_model, d_state)
            
            self.layers.append(layer)
        
        self.final_norm = nn.LayerNorm(d_model)
        
        # ì»´íŒŒì¼ ì¤€ë¹„
        self._compiled = False
        
        print(f"ğŸš€ OptimizedS6SSM initialized:")
        print(f"   Layers: {n_layers}")
        print(f"   Fast layers: {'âœ…' if use_fast_layers else 'âŒ'}")
        print(f"   Gradient checkpointing: {'âœ…' if use_gradient_checkpointing else 'âŒ'}")
    
    def compile_encoder(self):
        """ì¸ì½”ë” ì»´íŒŒì¼"""
        if not self._compiled:
            try:
                # ê° ë ˆì´ì–´ ì»´íŒŒì¼
                for layer in self.layers:
                    if hasattr(layer, 'compile_block'):
                        layer.compile_block()
                
                self._compiled = True
                print("ğŸš€ S6SSMEncoder compiled")
            except Exception as e:
                print(f"âš ï¸ S6SSMEncoder compilation failed: {e}")
    
    @torch.amp.autocast('cuda')
    def forward(self, x):
        """
        ğŸ”¥ ìµœì í™”ëœ ìˆœì „íŒŒ
        Args:
            x: (B, T, D) HuBERTì—ì„œ ì˜¨ íŠ¹ì„±
        Returns:
            (B, T, D) ì¸ì½”ë”©ëœ íŠ¹ì„±
        """
        if self.use_gradient_checkpointing and self.training:
            # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‚¬ìš©
            for layer in self.layers:
                x = checkpoint(layer, x, use_reentrant=False)
        else:
            # ì¼ë°˜ ìˆœì „íŒŒ
            for layer in self.layers:
                x = layer(x)
        
        return self.final_norm(x)

class ParallelS6Block(nn.Module):
    """
    ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”ëœ S6 Block
    - ì—¬ëŸ¬ í—¤ë“œ ë³‘ë ¬ ì²˜ë¦¬
    - ë” íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©
    """
    def __init__(self, d_model, d_state=64, num_heads=8):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.num_heads = num_heads
        self.d_head = d_model // num_heads
        
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        # ë©€í‹°í—¤ë“œ í”„ë¡œì ì…˜
        self.head_proj = nn.Linear(d_model, d_model * 3, bias=False)
        self.out_proj = nn.Linear(d_model, d_model, bias=False)
        
        # í—¤ë“œë³„ ìƒíƒœ ê³µê°„ íŒŒë¼ë¯¸í„°
        self.A_log = nn.Parameter(torch.randn(num_heads, self.d_head, d_state))
        self.D = nn.Parameter(torch.randn(num_heads, self.d_head))
        
        self.norm = nn.LayerNorm(d_model)
        
        self._init_parameters()
    
    def _init_parameters(self):
        """íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”"""
        nn.init.xavier_uniform_(self.head_proj.weight)
        nn.init.xavier_uniform_(self.out_proj.weight)
        nn.init.uniform_(self.A_log, -4.0, -1.0)
        nn.init.normal_(self.D, mean=1.0, std=0.1)
    
    def forward(self, x):
        """ë³‘ë ¬ ì²˜ë¦¬ ìˆœì „íŒŒ"""
        B, L, D = x.shape
        residual = x
        
        # ë©€í‹°í—¤ë“œ í”„ë¡œì ì…˜
        qkv = self.head_proj(x).view(B, L, 3, self.num_heads, self.d_head)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, L, d_head)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # ë³‘ë ¬ ì„ íƒì  ìŠ¤ìº” (ê°„ë‹¨í™”)
        # ì‹¤ì œë¡œëŠ” ê° í—¤ë“œë³„ë¡œ ì„ íƒì  ìŠ¤ìº”ì„ ë³‘ë ¬ ìˆ˜í–‰
        attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)
        attn = F.softmax(attn, dim=-1)
        out = torch.matmul(attn, v)
        
        # í—¤ë“œ ê²°í•©
        out = out.transpose(1, 2).contiguous().view(B, L, D)
        
        # ì¶œë ¥ í”„ë¡œì ì…˜
        x = self.out_proj(out)
        
        return self.norm(x + residual)

# ğŸš€ S6 ìµœì í™” ìœ í‹¸ë¦¬í‹°

def benchmark_s6_performance(d_model=768, seq_len=1000, batch_size=8, num_runs=10):
    """S6 ë¸”ë¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    import time
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # í…ŒìŠ¤íŠ¸í•  ë¸”ë¡ë“¤
    blocks = {
        'OptimizedS6': OptimizedS6Block(d_model).to(device),
        'FastS6': FastS6Block(d_model).to(device),
        'ParallelS6': ParallelS6Block(d_model).to(device)
    }
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    x = torch.randn(batch_size, seq_len, d_model, device=device)
    
    print(f"ğŸš€ S6 Performance Benchmark:")
    print(f"   Input shape: {x.shape}")
    print(f"   Device: {device}")
    print(f"   Runs: {num_runs}")
    
    results = {}
    
    for name, block in blocks.items():
        block.eval()
        
        # ì›Œë°ì—…
        with torch.no_grad():
            for _ in range(3):
                _ = block(x)
        
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        
        # ë²¤ì¹˜ë§ˆí¬
        times = []
        with torch.no_grad():
            for _ in range(num_runs):
                start_time = time.time()
                output = block(x)
                torch.cuda.synchronize() if torch.cuda.is_available() else None
                end_time = time.time()
                times.append(end_time - start_time)
        
        avg_time = sum(times) / len(times)
        throughput = batch_size * seq_len / avg_time
        
        results[name] = {
            'avg_time': avg_time,
            'throughput': throughput,
            'output_shape': output.shape
        }
        
        print(f"   {name}:")
        print(f"     Time: {avg_time*1000:.2f}ms")
        print(f"     Throughput: {throughput:.0f} tokens/sec")
        print(f"     Memory: {torch.cuda.memory_allocated()//1024//1024}MB" if torch.cuda.is_available() else "")
    
    return results

def create_adaptive_ssm_encoder(d_model=768, target_speed='balanced'):
    """
    ì ì‘ì  SSM ì¸ì½”ë” ìƒì„±
    speed: 'fast', 'balanced', 'quality'
    """
    configs = {
        'fast': {
            'n_layers': 2,
            'd_state': 32,
            'use_fast_layers': True,
            'use_gradient_checkpointing': False
        },
        'balanced': {
            'n_layers': 3,
            'd_state': 64,
            'use_fast_layers': False,
            'use_gradient_checkpointing': True
        },
        'quality': {
            'n_layers': 4,
            'd_state': 128,
            'use_fast_layers': False,
            'use_gradient_checkpointing': True
        }
    }
    
    config = configs.get(target_speed, configs['balanced'])
    
    encoder = OptimizedS6SSMEncoder(
        d_model=d_model,
        **config
    )
    
    print(f"ğŸ¯ Created {target_speed} SSM encoder")
    return encoder

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
S6SSMEncoder = OptimizedS6SSMEncoder