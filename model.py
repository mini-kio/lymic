import torch
import torch.nn as nn
from transformers import HubertModel
from ssm import S6SSMEncoder
from flow_matching import RectifiedFlow
import torch.nn.functional as F
import math

class F0Embedding(nn.Module):
    """F0 ì •ë³´ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì‹¤ì œ ìƒì„±ì— í™œìš©"""
    def __init__(self, d_model=768):
        super().__init__()
        # F0ì™€ VUVë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì„ë² ë”©
        self.f0_proj = nn.Sequential(
            nn.Linear(1, d_model // 4),
            nn.SiLU(),
            nn.Linear(d_model // 4, d_model // 2)
        )
        self.vuv_proj = nn.Sequential(
            nn.Linear(1, d_model // 4),
            nn.SiLU(), 
            nn.Linear(d_model // 4, d_model // 2)
        )
        
        # ê²°í•© ë„¤íŠ¸ì›Œí¬
        self.combine_proj = nn.Linear(d_model, d_model)
        
    def forward(self, f0, vuv, semitone_shift=0.0):
        """
        Args:
            f0: (B, T) ì •ê·œí™”ëœ F0 ê°’
            vuv: (B, T) voiced/unvoiced í”Œë˜ê·¸
            semitone_shift: float ë˜ëŠ” (B,) ì„¸ë¯¸í†¤ ì‹œí”„íŠ¸ (-12 ~ +12)
        Returns:
            (B, T, d_model) F0 ì„ë² ë”©
        """
        # ğŸµ ì„¸ë¯¸í†¤ ì‹œí”„íŠ¸ ì ìš©
        if semitone_shift != 0.0:
            f0_shifted = self.apply_semitone_shift(f0, vuv, semitone_shift)
        else:
            f0_shifted = f0
        
        f0_emb = self.f0_proj(f0_shifted.unsqueeze(-1))  # (B, T, d_model//2)
        vuv_emb = self.vuv_proj(vuv.unsqueeze(-1))  # (B, T, d_model//2)
        
        # F0ëŠ” voiced ì˜ì—­ì—ì„œë§Œ í™œì„±í™”
        f0_emb = f0_emb * vuv.unsqueeze(-1)
        
        # ê²°í•©
        combined = torch.cat([f0_emb, vuv_emb], dim=-1)  # (B, T, d_model)
        return self.combine_proj(combined)
    
    def apply_semitone_shift(self, f0, vuv, semitone_shift):
        """
        ğŸµ ì„¸ë¯¸í†¤ ì‹œí”„íŠ¸ ì ìš©
        Args:
            f0: (B, T) F0 ê°’ (Hz ë‹¨ìœ„ ë˜ëŠ” log scale)
            vuv: (B, T) voiced/unvoiced ë§ˆìŠ¤í¬
            semitone_shift: float ë˜ëŠ” (B,) ì„¸ë¯¸í†¤ ìˆ˜ (-12 ~ +12)
        Returns:
            (B, T) ì‹œí”„íŠ¸ëœ F0 ê°’
        """
        if isinstance(semitone_shift, (int, float)):
            if semitone_shift == 0.0:
                return f0
            semitone_shift = torch.tensor(semitone_shift, device=f0.device, dtype=f0.dtype)
        
        # ë°°ì¹˜ë³„ ì„œë¡œ ë‹¤ë¥¸ ì‹œí”„íŠ¸ ì§€ì›
        if semitone_shift.dim() == 0:
            semitone_shift = semitone_shift.unsqueeze(0).expand(f0.size(0))
        
        # Log scaleì—ì„œ ì„¸ë¯¸í†¤ ì‹œí”„íŠ¸ (ë” ì •í™•í•¨)
        # 1 semitone = log2(2^(1/12)) = 1/12 in log2 scale
        shift_factor = semitone_shift.unsqueeze(1) * (1.0 / 12.0)  # (B, 1)
        
        # F0ê°€ ì´ë¯¸ log scaleì´ë¼ê³  ê°€ì •í•˜ê³  ì‹œí”„íŠ¸
        f0_shifted = f0 + shift_factor
        
        # Voiced ì˜ì—­ì—ì„œë§Œ ì‹œí”„íŠ¸ ì ìš©
        f0_shifted = f0_shifted * vuv + f0 * (1 - vuv)
        
        return f0_shifted

class LoRALayer(nn.Module):
    """ê²½ëŸ‰í™”ëœ LoRA ë ˆì´ì–´"""
    def __init__(self, in_features, out_features, rank=16, alpha=16):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        
        # íš¨ìœ¨ì ì¸ ì´ˆê¸°í™”
        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * (1.0 / math.sqrt(rank)))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        
        self.scaling = alpha / rank
        
    def forward(self, x):
        lora_weight = self.lora_B @ self.lora_A * self.scaling
        return F.linear(x, lora_weight)

class SpeakerAdapter(nn.Module):
    """ìµœì í™”ëœ Speaker Adapter"""
    def __init__(self, d_model=768, adapter_dim=64):
        super().__init__()
        
        # ë” íš¨ìœ¨ì ì¸ êµ¬ì¡°
        self.adapter = nn.Sequential(
            nn.Linear(d_model, adapter_dim),
            nn.SiLU(),
            nn.Linear(adapter_dim, d_model)
        )
        
        self.layer_norm = nn.LayerNorm(d_model)
        
        # ì‘ì€ ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”
        for module in self.adapter:
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, std=0.01)
                nn.init.zeros_(module.bias)
    
    def forward(self, x):
        return self.layer_norm(x + self.adapter(x))

class VoiceConversionModel(nn.Module):
    def __init__(self, 
                 hubert_model_name="utter-project/mHuBERT-147",  # ğŸŒ ë‹¤êµ­ì–´ HuBERT-147
                 d_model=768,
                 ssm_layers=3,
                 flow_steps=20,  # ğŸ”¥ Rectified FlowëŠ” ë” ì ì€ ë‹¨ê³„ë¡œë„ ê³ í’ˆì§ˆ
                 n_speakers=256,
                 waveform_length=16384,
                 use_retrieval=True,
                 lora_rank=16,
                 adapter_dim=64,
                 use_f0_conditioning=True):  # ğŸµ F0 ì¡°ê±´ë¶€ ìƒì„±
        super().__init__()
        
        self.use_f0_conditioning = use_f0_conditioning
        
        # ğŸ”’ mHuBERT-147 (FROZEN) - ë‹¤êµ­ì–´ ì§€ì›
        self.hubert = HubertModel.from_pretrained(hubert_model_name)
        for param in self.hubert.parameters():
            param.requires_grad = False
        
        print(f"ğŸŒ Loaded mHuBERT-147: Multilingual speech representation model")
        print(f"   Model: {hubert_model_name}")
        print(f"   Hidden size: {self.hubert.config.hidden_size}")
        print(f"   Languages: 147+ languages supported")
            
        # ğŸ”’ SSM Encoder (FROZEN during fine-tuning)
        self.ssm_encoder = S6SSMEncoder(d_model=d_model, n_layers=ssm_layers)
        
        # ğŸ”¥ Rectified Flow (ë” íš¨ìœ¨ì )
        condition_dim = d_model
        if use_f0_conditioning:
            condition_dim += d_model  # F0 ì„ë² ë”© ì¶”ê°€
            
        self.rectified_flow = RectifiedFlow(
            dim=waveform_length,
            condition_dim=condition_dim,
            steps=flow_steps,
            hidden_dim=512  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ì¶•ì†Œ
        )
        
        # ğŸ”¥ TRAINABLE COMPONENTS
        self.speaker_embedding = nn.Embedding(n_speakers, d_model)
        self.speaker_adapter = SpeakerAdapter(d_model, adapter_dim)
        self.speaker_lora = LoRALayer(d_model, d_model, rank=lora_rank)
        
        # ğŸµ F0 ê´€ë ¨ ëª¨ë“ˆ
        if use_f0_conditioning:
            self.f0_embedding = F0Embedding(d_model)
            
        # F0/VUV ì˜ˆì¸¡ í—¤ë“œ (ë” íš¨ìœ¨ì )
        self.f0_proj = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.SiLU(),
            nn.Linear(128, 1)
        )
        
        self.vuv_proj = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.SiLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
        
        # ğŸ” Retrieval module
        self.use_retrieval = use_retrieval
        if use_retrieval:
            self.retrieval_module = RetrievalModule(d_model)
        
        # ğŸš€ ìµœì í™” í”Œë˜ê·¸
        self.enable_compile = True  # torch.compile í™œì„±í™”
        self._is_compiled = False
        
    def compile_model(self):
        """PyTorch 2.0 ì»´íŒŒì¼ ìµœì í™”"""
        if self.enable_compile and not self._is_compiled:
            try:
                # í•µì‹¬ ëª¨ë“ˆë“¤ ì»´íŒŒì¼
                self.ssm_encoder = torch.compile(self.ssm_encoder, mode='max-autotune')
                self.rectified_flow = torch.compile(self.rectified_flow, mode='max-autotune')
                if self.use_f0_conditioning:
                    self.f0_embedding = torch.compile(self.f0_embedding, mode='max-autotune')
                
                self._is_compiled = True
                print("ğŸš€ Model compiled for optimization")
            except Exception as e:
                print(f"âš ï¸ Compilation failed: {e}")
                
    def freeze_base_model(self):
        """Fine-tuningì„ ìœ„í•œ ê¸°ë³¸ ëª¨ë¸ ê³ ì •"""
        for param in self.ssm_encoder.parameters():
            param.requires_grad = False
        for param in self.rectified_flow.parameters():
            param.requires_grad = False
        print("ğŸ”’ Base model frozen for fine-tuning")
    
    def get_trainable_parameters(self):
        """í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ ë°˜í™˜"""
        trainable_params = []
        trainable_params.extend(self.speaker_embedding.parameters())
        trainable_params.extend(self.speaker_adapter.parameters())
        trainable_params.extend(self.speaker_lora.parameters())
        trainable_params.extend(self.f0_proj.parameters())
        trainable_params.extend(self.vuv_proj.parameters())
        
        if self.use_f0_conditioning:
            trainable_params.extend(self.f0_embedding.parameters())
        if self.use_retrieval:
            trainable_params.extend(self.retrieval_module.parameters())
            
        return trainable_params
    
    def _convert_to_mono(self, waveform):
        """ìŠ¤í…Œë ˆì˜¤ë¥¼ ëª¨ë…¸ë¡œ ë³€í™˜"""
        if waveform.dim() == 3:
            return waveform.mean(dim=1)
        return waveform
    
    def _interpolate_f0_to_content(self, f0, content_length):
        """F0 ê¸¸ì´ë¥¼ content ê¸¸ì´ì— ë§ì¶¤"""
        if f0.size(1) == content_length:
            return f0
        return F.interpolate(
            f0.unsqueeze(1), 
            size=content_length, 
            mode='linear', 
            align_corners=False
        ).squeeze(1)
    
    @torch.cuda.amp.autocast()  # ğŸ”¥ AMP ìë™ ì ìš©
    def forward(self, 
                source_waveform, 
                target_speaker_id, 
                target_waveform=None, 
                f0_target=None, 
                vuv_target=None, 
                semitone_shift=0.0,  # ğŸµ ì„¸ë¯¸í†¤ ì‹œí”„íŠ¸ ì¶”ê°€
                training=True,
                inference_method='fast_rectified',
                num_steps=8):
        """
        ğŸ”¥ ìµœì í™”ëœ forward pass with AMP
        """
        # ìŠ¤í…Œë ˆì˜¤ -> ëª¨ë…¸ ë³€í™˜
        if source_waveform.dim() == 3:
            source_mono = source_waveform.mean(dim=1)
        else:
            source_mono = source_waveform
            
        # ğŸ”’ HuBERT ì¶”ì¶œ (FP32 ìœ ì§€)
        with torch.cuda.amp.autocast(enabled=False):
            with torch.no_grad():
                hubert_output = self.hubert(source_mono.float())
                content_repr = hubert_output.last_hidden_state
        
        # ğŸ”’ SSM ì¸ì½”ë”©
        if training and hasattr(self, '_is_finetuning') and self._is_finetuning:
            with torch.no_grad():
                encoded_content = self.ssm_encoder(content_repr)
        else:
            encoded_content = self.ssm_encoder(content_repr)
        
        # ğŸ”¥ Speaker ì²˜ë¦¬
        speaker_emb = self.speaker_embedding(target_speaker_id)
        speaker_emb = speaker_emb.unsqueeze(1).expand(-1, encoded_content.size(1), -1)
        
        # LoRA ì ìš©
        speaker_emb_lora = self.speaker_lora(speaker_emb)
        
        # ê²°í•©
        condition = encoded_content + speaker_emb + speaker_emb_lora
        condition = self.speaker_adapter(condition)
        
        # ğŸµ F0 ì¡°ê±´ë¶€ ìƒì„±
        if self.use_f0_conditioning and f0_target is not None and vuv_target is not None:
            # F0 ê¸¸ì´ë¥¼ content ê¸¸ì´ì— ë§ì¶¤
            f0_resized = self._interpolate_f0_to_content(f0_target, encoded_content.size(1))
            vuv_resized = self._interpolate_f0_to_content(vuv_target, encoded_content.size(1))
            
            # F0 ì„ë² ë”© (ì„¸ë¯¸í†¤ ì‹œí”„íŠ¸ í¬í•¨)
            f0_emb = self.f0_embedding(f0_resized, vuv_resized, semitone_shift)
            
            # ì¡°ê±´ì— F0 ì •ë³´ ì¶”ê°€
            condition = torch.cat([condition, f0_emb], dim=-1)
        
        # ğŸ” Retrieval ê°•í™”
        if self.use_retrieval and hasattr(self, 'retrieval_module'):
            condition_pooled = condition.mean(dim=1)
            condition_pooled = self.retrieval_module.enhance(condition_pooled, target_speaker_id)
        else:
            condition_pooled = condition.mean(dim=1)
        
        if training and target_waveform is not None:
            # ğŸ”¥ í›ˆë ¨ ëª¨ë“œ
            target_mono = self._convert_to_mono(target_waveform)
            
            # Rectified Flow ì†ì‹¤
            if hasattr(self, '_is_finetuning') and self._is_finetuning:
                with torch.no_grad():
                    flow_loss = self.rectified_flow.compute_loss(target_mono, condition_pooled)
                flow_loss = flow_loss.detach()
            else:
                flow_loss = self.rectified_flow.compute_loss(target_mono, condition_pooled)
            
            results = {'flow_loss': flow_loss}
            
            # ğŸµ ë³´ì¡° ì†ì‹¤ë“¤
            if f0_target is not None:
                f0_pred = self.f0_proj(encoded_content).squeeze(-1)
                f0_pred_resized = self._interpolate_f0_to_content(f0_pred, f0_target.size(1))
                f0_loss = F.mse_loss(f0_pred_resized, f0_target)
                results['f0_loss'] = f0_loss
                
            if vuv_target is not None:
                vuv_pred = self.vuv_proj(encoded_content).squeeze(-1)
                vuv_pred_resized = self._interpolate_f0_to_content(vuv_pred, vuv_target.size(1))
                vuv_loss = F.binary_cross_entropy(vuv_pred_resized, vuv_target.float())
                results['vuv_loss'] = vuv_loss
            
            # ì´ ì†ì‹¤
            total_loss = torch.tensor(0.0, device=flow_loss.device, dtype=flow_loss.dtype)
            if not (hasattr(self, '_is_finetuning') and self._is_finetuning):
                total_loss += flow_loss
            if 'f0_loss' in results:
                total_loss += 0.1 * results['f0_loss']
            if 'vuv_loss' in results:
                total_loss += 0.1 * results['vuv_loss']
                
            results['total_loss'] = total_loss
            return results
            
        else:
            # ğŸš€ ì¶”ë¡  ëª¨ë“œ - Rectified Flow ìƒ˜í”Œë§
            converted_waveform = self.rectified_flow.sample(
                condition=condition_pooled,
                num_steps=num_steps,
                method=inference_method
            )
            
            # F0/VUV ì˜ˆì¸¡
            f0_pred = self.f0_proj(encoded_content).squeeze(-1)
            vuv_pred = self.vuv_proj(encoded_content).squeeze(-1)
            
            return {
                'converted_waveform': converted_waveform,
                'f0_pred': f0_pred,
                'vuv_pred': vuv_pred
            }

class RetrievalModule(nn.Module):
    """ìµœì í™”ëœ ê²€ìƒ‰ ëª¨ë“ˆ"""
    def __init__(self, feature_dim=768, k=5):
        super().__init__()
        self.feature_dim = feature_dim
        self.k = k
        
        # ë” íš¨ìœ¨ì ì¸ ê°•í™” ë„¤íŠ¸ì›Œí¬
        self.enhance_net = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.SiLU(),
            nn.Dropout(0.05),  # ë‚®ì€ ë“œë¡­ì•„ì›ƒ
            nn.Linear(feature_dim, feature_dim)
        )
        
        # í›ˆë ¨ íŠ¹ì„± ì €ì¥ì†Œ
        self.register_buffer('training_features', torch.empty(0, feature_dim))
        self.register_buffer('speaker_ids', torch.empty(0, dtype=torch.long))
        
        # ìºì‹œ ìµœì í™”
        self._cache = {}
        self._cache_size = 100
        
    def add_training_features(self, features, speaker_ids):
        """í›ˆë ¨ íŠ¹ì„± ì¶”ê°€"""
        features = features.detach().cpu()
        speaker_ids = speaker_ids.detach().cpu()
        
        self.training_features = torch.cat([self.training_features, features], dim=0)
        self.speaker_ids = torch.cat([self.speaker_ids, speaker_ids], dim=0)
        
        # ìºì‹œ ì´ˆê¸°í™”
        self._cache.clear()
    
    @torch.cuda.amp.autocast()
    def enhance(self, content_features, target_speaker_id):
        """ì»¨í…ì¸  íŠ¹ì„± ê°•í™”"""
        if self.training_features.size(0) == 0:
            return content_features
        
        B = content_features.size(0)
        device = content_features.device
        enhanced_features = []
        
        for i in range(B):
            query = content_features[i:i+1]
            spk_id = target_speaker_id[i].item()
            
            # ìºì‹œ í™•ì¸
            cache_key = f"{spk_id}_{hash(query.cpu().numpy().tobytes())}"
            if cache_key in self._cache:
                enhanced_features.append(self._cache[cache_key])
                continue
            
            # ë™ì¼ í™”ì íŠ¹ì„± ì°¾ê¸°
            same_speaker_mask = (self.speaker_ids == spk_id)
            if same_speaker_mask.sum() > 0:
                candidate_features = self.training_features[same_speaker_mask].to(device)
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarities = F.cosine_similarity(
                    query.unsqueeze(1),
                    candidate_features.unsqueeze(0),
                    dim=-1
                ).squeeze(0)
                
                # Top-k ì„ íƒ
                k = min(self.k, similarities.size(0))
                _, top_indices = similarities.topk(k)
                retrieved_features = candidate_features[top_indices].mean(dim=0, keepdim=True)
                
                # ê°•í™”
                combined = torch.cat([query, retrieved_features], dim=-1)
                enhanced = self.enhance_net(combined)
                
                # ìºì‹œ ì €ì¥ (í¬ê¸° ì œí•œ)
                if len(self._cache) < self._cache_size:
                    self._cache[cache_key] = enhanced
                
                enhanced_features.append(enhanced)
            else:
                enhanced_features.append(query)
        
        return torch.cat(enhanced_features, dim=0)