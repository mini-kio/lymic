import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
import numpy as np
from pathlib import Path
import wandb
from tqdm import tqdm
import time
import warnings

from model import VoiceConversionModel
from utils import VoiceConversionDataset, collate_fn
from flow_matching import FlowScheduler

warnings.filterwarnings("ignore", category=UserWarning)

class OptimizedVoiceConversionTrainer:
    """
    ğŸš€ ìµœì í™”ëœ Voice Conversion Trainer
    - AMP FP16 í˜¼í•© ì •ë°€ë„
    - Rectified Flow 
    - F0 ì¡°ê±´ë¶€ ìƒì„±
    - ì»´íŒŒì¼ ìµœì í™”
    """
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ğŸ”¥ AMP ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™”
        self.scaler = GradScaler()
        self.use_amp = config.get('use_amp', True) and torch.cuda.is_available()
        
        print(f"ğŸš€ Initializing trainer with:")
        print(f"   Device: {self.device}")
        print(f"   AMP FP16: {'âœ… Enabled' if self.use_amp else 'âŒ Disabled'}")
        
        # ğŸ”¥ ëª¨ë¸ ì´ˆê¸°í™”
        self.model = VoiceConversionModel(
            d_model=config.get('d_model', 768),
            ssm_layers=config.get('ssm_layers', 3),
            flow_steps=config.get('flow_steps', 20),  # Rectified FlowëŠ” ë” ì ì€ ë‹¨ê³„
            n_speakers=config.get('n_speakers', 256),
            waveform_length=config.get('waveform_length', 16384),
            use_retrieval=config.get('use_retrieval', True),
            lora_rank=config.get('lora_rank', 16),
            adapter_dim=config.get('adapter_dim', 64),
            use_f0_conditioning=config.get('use_f0_conditioning', True)
        ).to(self.device)
        
        # ğŸš€ ëª¨ë¸ ì»´íŒŒì¼ (PyTorch 2.0+)
        if config.get('compile_model', True):
            self.model.compile_model()
        
        print(f"ğŸµ F0 conditioning: {'âœ… Enabled' if config.get('use_f0_conditioning', True) else 'âŒ Disabled'}")
        print(f"ğŸ” Retrieval: {'âœ… Enabled' if config.get('use_retrieval', True) else 'âŒ Disabled'}")
        
        # ğŸ”¥ ìµœì í™”ëœ ì˜µí‹°ë§ˆì´ì €
        self._setup_optimizer()
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=config.get('lr', 1e-4),
            total_steps=config.get('max_epochs', 80) * config.get('steps_per_epoch', 1000),
            pct_start=0.1,  # 10% ì›œì—…
            div_factor=25,  # ì´ˆê¸° LR = max_lr / 25
            final_div_factor=10000  # ìµœì¢… LR = max_lr / 10000
        )
        
        # ì†ì‹¤ ê°€ì¤‘ì¹˜
        self.flow_weight = config.get('flow_weight', 1.0)
        self.f0_weight = config.get('f0_weight', 0.1)
        self.vuv_weight = config.get('vuv_weight', 0.1)
        
        # ğŸ”¥ ë™ì  ìŠ¤ì¼€ì¤„ë§
        self.flow_scheduler = FlowScheduler()
        
        # ë¡œê¹…
        if config.get('use_wandb', False):
            wandb.init(
                project="voice-conversion-optimized",
                config=config,
                name=f"optimized_{config.get('experiment_name', 'default')}"
            )
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.training_times = []
        self.memory_usage = []
        
    def _setup_optimizer(self):
        """ìµœì í™”ëœ ì˜µí‹°ë§ˆì´ì € ì„¤ì •"""
        # íŒŒë¼ë¯¸í„° ê·¸ë£¹ ë¶„ë¦¬
        no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']
        
        optimizer_grouped_parameters = [
            {
                'params': [p for n, p in self.model.named_parameters() 
                          if not any(nd in n for nd in no_decay) and p.requires_grad],
                'weight_decay': self.config.get('weight_decay', 1e-5),
                'lr': self.config.get('lr', 1e-4)
            },
            {
                'params': [p for n, p in self.model.named_parameters() 
                          if any(nd in n for nd in no_decay) and p.requires_grad],
                'weight_decay': 0.0,
                'lr': self.config.get('lr', 1e-4)
            }
        ]
        
        # ğŸ”¥ AdamW with fused optimization
        self.optimizer = optim.AdamW(
            optimizer_grouped_parameters,
            lr=self.config.get('lr', 1e-4),
            betas=(0.9, 0.999),
            eps=1e-8,
            fused=True if torch.cuda.is_available() else False  # CUDA ìµœì í™”
        )
        
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        
        print(f"âš™ï¸ Optimizer setup:")
        print(f"   Total parameters: {total_params:,}")
        print(f"   Trainable parameters: {trainable_params:,}")
        print(f"   Trainable ratio: {trainable_params/total_params*100:.1f}%")
        print(f"   Fused AdamW: {'âœ… Enabled' if torch.cuda.is_available() else 'âŒ Disabled'}")
    
    def train_epoch(self, train_loader, epoch):
        """ğŸ”¥ AMP FP16 ìµœì í™”ëœ í›ˆë ¨ ì—í¬í¬"""
        self.model.train()
        
        total_loss = 0
        flow_loss_sum = 0
        f0_loss_sum = 0
        vuv_loss_sum = 0
        
        # ë™ì  ì¶”ë¡  ë‹¨ê³„ ê³„ì‚°
        inference_steps = self.flow_scheduler.get_progressive_schedule(
            max_steps=self.config.get('flow_steps', 20),
            current_epoch=epoch,
            total_epochs=self.config.get('max_epochs', 80)
        )
        
        epoch_start_time = time.time()
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
        for batch_idx, batch in enumerate(pbar):
            batch_start_time = time.time()
            
            # ë°ì´í„° GPU ì´ë™
            source_waveform = batch['source_waveform'].to(self.device, non_blocking=True)
            target_waveform = batch['target_waveform'].to(self.device, non_blocking=True)
            target_speaker_id = batch['target_speaker_id'].to(self.device, non_blocking=True)
            
            # F0/VUV ë°ì´í„°
            f0_target = batch.get('f0_target')
            vuv_target = batch.get('vuv_target')
            if f0_target is not None:
                f0_target = f0_target.to(self.device, non_blocking=True)
            if vuv_target is not None:
                vuv_target = vuv_target.to(self.device, non_blocking=True)
            
            # ğŸ”¥ AMP í˜¼í•© ì •ë°€ë„ ìˆœì „íŒŒ
            with autocast(enabled=self.use_amp):
                outputs = self.model(
                    source_waveform=source_waveform,
                    target_speaker_id=target_speaker_id,
                    target_waveform=target_waveform,
                    f0_target=f0_target,
                    vuv_target=vuv_target,
                    training=True,
                    inference_method='fast_rectified',
                    num_steps=inference_steps
                )
                
                loss = outputs['total_loss']
                flow_loss = outputs['flow_loss']
            
            # ğŸ”¥ AMP ìŠ¤ì¼€ì¼ëœ ì—­ì „íŒŒ
            self.optimizer.zero_grad(set_to_none=True)  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
            if self.use_amp:
                self.scaler.scale(loss).backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ìŠ¤ì¼€ì¼ëœ ê·¸ë˜ë””ì–¸íŠ¸ì—)
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
            
            self.scheduler.step()
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            total_loss += loss.item()
            flow_loss_sum += flow_loss.item()
            
            aux_loss_sum = 0
            if 'f0_loss' in outputs:
                f0_loss_sum += outputs['f0_loss'].item()
                aux_loss_sum += outputs['f0_loss'].item()
            if 'vuv_loss' in outputs:
                vuv_loss_sum += outputs['vuv_loss'].item()
                aux_loss_sum += outputs['vuv_loss'].item()
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            batch_time = time.time() - batch_start_time
            
            # ì§„í–‰ë¥  í‘œì‹œ
            pbar_dict = {
                'Loss': f'{loss.item():.4f}',
                'Flow': f'{flow_loss.item():.4f}',
                'Steps': f'{inference_steps}',
                'LR': f'{self.optimizer.param_groups[0]["lr"]:.2e}',
                'Time': f'{batch_time:.2f}s'
            }
            
            if aux_loss_sum > 0:
                pbar_dict['F0+VUV'] = f'{aux_loss_sum:.4f}'
            
            if self.use_amp:
                pbar_dict['Scale'] = f'{self.scaler.get_scale():.0f}'
            
            pbar.set_postfix(pbar_dict)
            
            # Wandb ë¡œê¹…
            if self.config.get('use_wandb', False) and batch_idx % 50 == 0:
                log_dict = {
                    'train/loss': loss.item(),
                    'train/flow_loss': flow_loss.item(),
                    'train/lr': self.optimizer.param_groups[0]['lr'],
                    'train/inference_steps': inference_steps,
                    'train/batch_time': batch_time,
                    'train/epoch': epoch + 1
                }
                
                if 'f0_loss' in outputs:
                    log_dict['train/f0_loss'] = outputs['f0_loss'].item()
                if 'vuv_loss' in outputs:
                    log_dict['train/vuv_loss'] = outputs['vuv_loss'].item()
                if self.use_amp:
                    log_dict['train/grad_scale'] = self.scaler.get_scale()
                
                wandb.log(log_dict)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ (ê°€ë”ì”©)
            if batch_idx % 100 == 0 and torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated(self.device) / 1024**3
                self.memory_usage.append(memory_allocated)
        
        # ì—í¬í¬ ì„±ëŠ¥ ê¸°ë¡
        epoch_time = time.time() - epoch_start_time
        self.training_times.append(epoch_time)
        
        num_batches = len(train_loader)
        metrics = {
            'total_loss': total_loss / num_batches,
            'flow_loss': flow_loss_sum / num_batches,
            'f0_loss': f0_loss_sum / num_batches if f0_loss_sum > 0 else 0,
            'vuv_loss': vuv_loss_sum / num_batches if vuv_loss_sum > 0 else 0,
            'epoch_time': epoch_time,
            'inference_steps': inference_steps
        }
        
        return metrics
    
    @torch.no_grad()
    def validate(self, val_loader, epoch):
        """ğŸš€ ìµœì í™”ëœ ê²€ì¦"""
        self.model.eval()
        
        total_loss = 0
        flow_loss_sum = 0
        f0_loss_sum = 0
        vuv_loss_sum = 0
        
        # ê²€ì¦ì€ ë” ì ì€ ë‹¨ê³„ë¡œ
        val_steps = max(4, self.config.get('flow_steps', 20) // 4)
        
        with torch.cuda.amp.autocast(enabled=self.use_amp):
            for batch in tqdm(val_loader, desc="Validation", leave=False):
                source_waveform = batch['source_waveform'].to(self.device, non_blocking=True)
                target_waveform = batch['target_waveform'].to(self.device, non_blocking=True)
                target_speaker_id = batch['target_speaker_id'].to(self.device, non_blocking=True)
                
                f0_target = batch.get('f0_target')
                vuv_target = batch.get('vuv_target')
                if f0_target is not None:
                    f0_target = f0_target.to(self.device, non_blocking=True)
                if vuv_target is not None:
                    vuv_target = vuv_target.to(self.device, non_blocking=True)
                
                outputs = self.model(
                    source_waveform=source_waveform,
                    target_speaker_id=target_speaker_id,
                    target_waveform=target_waveform,
                    f0_target=f0_target,
                    vuv_target=vuv_target,
                    training=True,
                    inference_method='fast_rectified',
                    num_steps=val_steps
                )
                
                loss = outputs['total_loss']
                flow_loss = outputs['flow_loss']
                
                total_loss += loss.item()
                flow_loss_sum += flow_loss.item()
                
                if 'f0_loss' in outputs:
                    f0_loss_sum += outputs['f0_loss'].item()
                if 'vuv_loss' in outputs:
                    vuv_loss_sum += outputs['vuv_loss'].item()
        
        num_batches = len(val_loader)
        val_metrics = {
            'total_loss': total_loss / num_batches,
            'flow_loss': flow_loss_sum / num_batches,
            'f0_loss': f0_loss_sum / num_batches if f0_loss_sum > 0 else 0,
            'vuv_loss': vuv_loss_sum / num_batches if vuv_loss_sum > 0 else 0
        }
        
        if self.config.get('use_wandb', False):
            wandb.log({f'val/{k}': v for k, v in val_metrics.items()})
        
        return val_metrics
    
    def train(self, train_loader, val_loader=None):
        """ë©”ì¸ í›ˆë ¨ ë£¨í”„"""
        print(f"\nğŸš€ Starting optimized training:")
        print(f"   Rectified Flow steps: {self.config.get('flow_steps', 20)}")
        print(f"   AMP FP16: {'âœ…' if self.use_amp else 'âŒ'}")
        print(f"   Model compilation: {'âœ…' if self.config.get('compile_model', True) else 'âŒ'}")
        
        best_val_loss = float('inf')
        
        for epoch in range(self.config.get('max_epochs', 80)):
            epoch_start = time.time()
            
            print(f"\nğŸ”¥ Epoch {epoch+1}/{self.config.get('max_epochs', 80)}")
            
            # ê²€ìƒ‰ ë°ì´í„° ìˆ˜ì§‘ (ì²« ì—í¬í¬)
            if epoch == 0 and self.model.use_retrieval:
                self._collect_retrieval_data(train_loader)
            
            # í›ˆë ¨
            train_metrics = self.train_epoch(train_loader, epoch)
            
            print(f"ğŸ“Š Train - Loss: {train_metrics['total_loss']:.4f}, " +
                  f"Flow: {train_metrics['flow_loss']:.4f}, " +
                  f"Time: {train_metrics['epoch_time']:.1f}s")
            
            if train_metrics['f0_loss'] > 0:
                print(f"        ğŸµ F0: {train_metrics['f0_loss']:.4f}, " +
                      f"VUV: {train_metrics['vuv_loss']:.4f}")
            
            # ê²€ì¦
            if val_loader is not None:
                val_metrics = self.validate(val_loader, epoch)
                print(f"ğŸ“Š Val   - Loss: {val_metrics['total_loss']:.4f}, " +
                      f"Flow: {val_metrics['flow_loss']:.4f}")
                
                if val_metrics['f0_loss'] > 0:
                    print(f"        ğŸµ F0: {val_metrics['f0_loss']:.4f}, " +
                          f"VUV: {val_metrics['vuv_loss']:.4f}")
                
                # ìµœê³  ëª¨ë¸ ì €ì¥
                if val_metrics['total_loss'] < best_val_loss:
                    best_val_loss = val_metrics['total_loss']
                    self.save_checkpoint(f"best_model_epoch_{epoch+1}.pt", epoch)
                    print(f"ğŸ’¾ New best model! (Loss: {best_val_loss:.4f})")
            
            # ì •ê¸° ì²´í¬í¬ì¸íŠ¸
            if (epoch + 1) % self.config.get('save_every', 10) == 0:
                self.save_checkpoint(f"checkpoint_epoch_{epoch+1}.pt", epoch)
            
            # ì„±ëŠ¥ ë¦¬í¬íŠ¸
            if (epoch + 1) % 20 == 0:
                self._print_performance_stats(epoch + 1)
        
        print(f"\nğŸ‰ Training completed!")
        self._print_final_stats()
    
    def _collect_retrieval_data(self, train_loader):
        """ê²€ìƒ‰ ëª¨ë“ˆìš© ë°ì´í„° ìˆ˜ì§‘"""
        if not self.model.use_retrieval:
            return
        
        print("ğŸ” Collecting retrieval features...")
        self.model.eval()
        
        with torch.no_grad(), torch.cuda.amp.autocast(enabled=self.use_amp):
            for batch in tqdm(train_loader, desc="Collecting features", leave=False):
                source_waveform = batch['source_waveform'].to(self.device, non_blocking=True)
                target_speaker_id = batch['target_speaker_id'].to(self.device, non_blocking=True)
                
                # HuBERT íŠ¹ì„± ì¶”ì¶œ
                if source_waveform.dim() == 3:
                    source_mono = source_waveform.mean(dim=1)
                else:
                    source_mono = source_waveform
                
                hubert_output = self.model.hubert(source_mono)
                content_repr = hubert_output.last_hidden_state
                encoded_content = self.model.ssm_encoder(content_repr)
                
                # ìŠ¤í”¼ì»¤ ì„ë² ë”©
                speaker_emb = self.model.speaker_embedding(target_speaker_id)
                speaker_emb = speaker_emb.unsqueeze(1).expand(-1, encoded_content.size(1), -1)
                
                # ê²°í•© ë° í’€ë§
                condition = encoded_content + speaker_emb
                condition = self.model.speaker_adapter(condition)
                condition_pooled = condition.mean(dim=1)
                
                # ê²€ìƒ‰ ëª¨ë“ˆì— ì¶”ê°€
                self.model.retrieval_module.add_training_features(
                    condition_pooled, target_speaker_id
                )
        
        print("âœ… Retrieval data collection completed")
        self.model.train()
    
    def save_checkpoint(self, filename, epoch):
        """ìµœì í™”ëœ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'scaler_state_dict': self.scaler.state_dict() if self.use_amp else None,
            'config': self.config,
            'training_times': self.training_times,
            'memory_usage': self.memory_usage
        }
        torch.save(checkpoint, filename)
        print(f"ğŸ’¾ Checkpoint saved: {filename}")
    
    def _print_performance_stats(self, epoch):
        """ì„±ëŠ¥ í†µê³„ ì¶œë ¥"""
        if self.training_times:
            avg_time = sum(self.training_times[-10:]) / min(10, len(self.training_times))
            print(f"âš¡ Avg epoch time (last 10): {avg_time:.1f}s")
        
        if self.memory_usage and torch.cuda.is_available():
            max_memory = max(self.memory_usage[-50:]) if len(self.memory_usage) >= 50 else max(self.memory_usage)
            print(f"ğŸ–¥ï¸ Peak GPU memory: {max_memory:.2f} GB")
    
    def _print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        print(f"\nğŸ“Š Final Training Statistics:")
        if self.training_times:
            total_time = sum(self.training_times)
            avg_time = total_time / len(self.training_times)
            print(f"   Total training time: {total_time/3600:.2f} hours")
            print(f"   Average epoch time: {avg_time:.1f}s")
        
        if self.memory_usage:
            avg_memory = sum(self.memory_usage) / len(self.memory_usage)
            max_memory = max(self.memory_usage)
            print(f"   Average GPU memory: {avg_memory:.2f} GB")
            print(f"   Peak GPU memory: {max_memory:.2f} GB")
        
        print(f"   AMP FP16 used: {'Yes' if self.use_amp else 'No'}")
        print(f"   Model compiled: {'Yes' if self.config.get('compile_model', True) else 'No'}")

def main():
    """ìµœì í™”ëœ í›ˆë ¨ ë©”ì¸ í•¨ìˆ˜"""
    config = {
        # ğŸ”¥ ìµœì í™” ì„¤ì •
        'use_amp': True,  # AMP FP16 í™œì„±í™”
        'compile_model': True,  # PyTorch 2.0 ì»´íŒŒì¼
        
        # ëª¨ë¸ ì„¤ì •
        'd_model': 768,
        'ssm_layers': 3,
        'flow_steps': 20,  # Rectified Flowë¡œ ë” ì ì€ ë‹¨ê³„
        'n_speakers': 256,
        'waveform_length': 16384,
        'use_retrieval': True,
        'lora_rank': 16,
        'adapter_dim': 64,
        'use_f0_conditioning': True,  # ğŸµ F0 ì¡°ê±´ë¶€ ìƒì„±
        
        # í›ˆë ¨ ì„¤ì •
        'batch_size': 12,  # FP16ìœ¼ë¡œ ë” í° ë°°ì¹˜ ê°€ëŠ¥
        'lr': 1e-4,
        'weight_decay': 1e-5,
        'max_epochs': 80,
        'save_every': 10,
        
        # ë°ì´í„° ì„¤ì •
        'data_dir': './data',
        'sample_rate': 44100,
        'channels': 2,
        'extract_f0': True,  # F0 ì¶”ì¶œ í™œì„±í™”
        'f0_method': 'pyin',
        'f0_weight': 0.1,
        'vuv_weight': 0.1,
        
        # ë¡œê¹…
        'use_wandb': False,
        'experiment_name': 'rectified_flow_f0'
    }
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    print("ğŸ“ Loading optimized dataset...")
    
    if Path(config['data_dir']).is_dir() and not (Path(config['data_dir']) / 'train').exists():
        full_dataset = VoiceConversionDataset(
            data_dir=config['data_dir'],
            sample_rate=config['sample_rate'],
            waveform_length=config['waveform_length'],
            channels=config['channels'],
            extract_f0=config['extract_f0'],
            hop_length=512,
            f0_method=config['f0_method']
        )
        
        total_size = len(full_dataset)
        train_size = int(0.8 * total_size)
        val_size = total_size - train_size
        
        train_dataset, val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size]
        )
        
    else:
        train_dataset = VoiceConversionDataset(
            data_dir=Path(config['data_dir']) / 'train',
            sample_rate=config['sample_rate'],
            waveform_length=config['waveform_length'],
            channels=config['channels'],
            extract_f0=config['extract_f0'],
            hop_length=512,
            f0_method=config['f0_method']
        )
        
        val_dataset = VoiceConversionDataset(
            data_dir=Path(config['data_dir']) / 'val',
            sample_rate=config['sample_rate'],
            waveform_length=config['waveform_length'],
            channels=config['channels'],
            extract_f0=config['extract_f0'],
            hop_length=512,
            f0_method=config['f0_method']
        )
    
    # ë°ì´í„°ì…‹ ì •ë³´
    if hasattr(train_dataset, 'dataset'):
        dataset_info = train_dataset.dataset.get_speaker_info()
        train_dataset.dataset.print_sample_pairs()
    else:
        dataset_info = train_dataset.get_speaker_info()
        train_dataset.print_sample_pairs()
    
    # í™”ì ìˆ˜ ì—…ë°ì´íŠ¸
    config['n_speakers'] = dataset_info['total_speakers']
    config['steps_per_epoch'] = len(train_dataset) // config['batch_size']
    
    print(f"\nğŸ¯ Optimized Dataset Info:")
    print(f"   ğŸ‘¥ Speakers: {dataset_info['total_speakers']}")
    print(f"   ğŸ“Š Training pairs: {len(train_dataset)}")
    print(f"   ğŸ” Validation pairs: {len(val_dataset)}")
    print(f"   ğŸµ F0 conditioning: âœ… Enabled")
    print(f"   ğŸš€ Rectified Flow: âœ… Enabled")
    
    # ğŸ”¥ ìµœì í™”ëœ ë°ì´í„° ë¡œë”
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=8,  # ë” ë§ì€ ì›Œì»¤
        pin_memory=True,
        persistent_workers=True,  # ì›Œì»¤ ì¬ì‚¬ìš©
        collate_fn=collate_fn,
        prefetch_factor=2,  # ë¯¸ë¦¬ ê°€ì ¸ì˜¤ê¸°
        drop_last=True  # ì•ˆì •ì ì¸ ë°°ì¹˜ í¬ê¸°
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True,
        collate_fn=collate_fn,
        drop_last=False
    )
    
    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ë° í›ˆë ¨
    trainer = OptimizedVoiceConversionTrainer(config)
    trainer.train(train_loader, val_loader)

if __name__ == "__main__":
    main()