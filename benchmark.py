"""
ğŸš€ ê³ ê¸‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬
flow_matching.pyì™€ ssm.py ëª¨ë“ˆë“¤ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
"""
import torch
import torch.nn as nn
import time
import psutil
import os
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import numpy as np

from flow_matching import RectifiedFlow, FlowScheduler
from ssm import OptimizedS6Block, FastS6Block, ParallelS6Block, create_adaptive_ssm_encoder

class PerformanceBenchmark:
    """ğŸš€ ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.results = {}
        
    def benchmark_flow_matching(self, 
                               dims=[1024, 4096, 16384],
                               batch_sizes=[2, 4, 8],
                               num_runs=10):
        """Flow Matching ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ§ª Flow Matching Benchmark Started...")
        
        flow_results = {}
        
        for dim in dims:
            for batch_size in batch_sizes:
                key = f"dim_{dim}_batch_{batch_size}"
                print(f"  Testing {key}...")
                
                # ëª¨ë¸ ì´ˆê¸°í™”
                flow = RectifiedFlow(
                    dim=dim, 
                    condition_dim=768, 
                    hidden_dim=min(512, dim//32)
                ).to(self.device)
                
                # í…ŒìŠ¤íŠ¸ ë°ì´í„°
                condition = torch.randn(batch_size, 768, device=self.device)
                x1 = torch.randn(batch_size, dim, device=self.device)
                
                # ì›Œë°ì—…
                for _ in range(3):
                    with torch.no_grad():
                        _ = flow.compute_loss(x1, condition)
                        _ = flow.sample(condition, num_steps=4)
                
                torch.cuda.synchronize() if torch.cuda.is_available() else None
                
                # ì†ì‹¤ ê³„ì‚° ë²¤ì¹˜ë§ˆí¬
                loss_times = []
                for _ in range(num_runs):
                    start_time = time.time()
                    loss = flow.compute_loss(x1, condition)
                    loss.backward()
                    flow.zero_grad()
                    torch.cuda.synchronize() if torch.cuda.is_available() else None
                    loss_times.append(time.time() - start_time)
                
                # ìƒ˜í”Œë§ ë²¤ì¹˜ë§ˆí¬
                sample_times = []
                with torch.no_grad():
                    for _ in range(num_runs):
                        start_time = time.time()
                        samples = flow.sample(condition, num_steps=4)
                        torch.cuda.synchronize() if torch.cuda.is_available() else None
                        sample_times.append(time.time() - start_time)
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                memory_mb = torch.cuda.memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0
                
                flow_results[key] = {
                    'loss_time_avg': np.mean(loss_times),
                    'loss_time_std': np.std(loss_times),
                    'sample_time_avg': np.mean(sample_times),
                    'sample_time_std': np.std(sample_times),
                    'memory_mb': memory_mb,
                    'dim': dim,
                    'batch_size': batch_size
                }
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del flow, condition, x1
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        self.results['flow_matching'] = flow_results
        print("âœ… Flow Matching Benchmark Completed")
        
    def benchmark_ssm_variants(self,
                              d_models=[256, 512, 768],
                              seq_lens=[100, 500, 1000],
                              batch_size=4,
                              num_runs=10):
        """SSM ë³€í˜•ë“¤ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ§ª SSM Variants Benchmark Started...")
        
        ssm_results = {}
        
        # í…ŒìŠ¤íŠ¸í•  SSM ë¸”ë¡ë“¤
        block_types = {
            'OptimizedS6': OptimizedS6Block,
            'FastS6': FastS6Block,
            'ParallelS6': ParallelS6Block
        }
        
        for d_model in d_models:
            for seq_len in seq_lens:
                for block_name, block_class in block_types.items():
                    key = f"{block_name}_d{d_model}_seq{seq_len}"
                    print(f"  Testing {key}...")
                    
                    try:
                        # ë¸”ë¡ ì´ˆê¸°í™”
                        if block_name == 'ParallelS6':
                            block = block_class(d_model, num_heads=min(8, d_model//64)).to(self.device)
                        else:
                            block = block_class(d_model).to(self.device)
                        
                        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
                        x = torch.randn(batch_size, seq_len, d_model, device=self.device)
                        
                        # ì›Œë°ì—…
                        for _ in range(3):
                            with torch.no_grad():
                                _ = block(x)
                        
                        torch.cuda.synchronize() if torch.cuda.is_available() else None
                        
                        # ìˆœì „íŒŒ ë²¤ì¹˜ë§ˆí¬
                        forward_times = []
                        for _ in range(num_runs):
                            start_time = time.time()
                            output = block(x)
                            torch.cuda.synchronize() if torch.cuda.is_available() else None
                            forward_times.append(time.time() - start_time)
                        
                        # ì—­ì „íŒŒ í¬í•¨ ë²¤ì¹˜ë§ˆí¬
                        backward_times = []
                        for _ in range(num_runs):
                            x_grad = x.clone().requires_grad_(True)
                            start_time = time.time()
                            output = block(x_grad)
                            loss = output.sum()
                            loss.backward()
                            torch.cuda.synchronize() if torch.cuda.is_available() else None
                            backward_times.append(time.time() - start_time)
                            block.zero_grad()
                        
                        # ì²˜ë¦¬ëŸ‰ ê³„ì‚°
                        tokens_per_sec = (batch_size * seq_len) / np.mean(forward_times)
                        
                        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                        memory_mb = torch.cuda.memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0
                        
                        ssm_results[key] = {
                            'forward_time_avg': np.mean(forward_times),
                            'forward_time_std': np.std(forward_times),
                            'backward_time_avg': np.mean(backward_times),
                            'backward_time_std': np.std(backward_times),
                            'tokens_per_sec': tokens_per_sec,
                            'memory_mb': memory_mb,
                            'd_model': d_model,
                            'seq_len': seq_len,
                            'block_type': block_name
                        }
                        
                        # ë©”ëª¨ë¦¬ ì •ë¦¬
                        del block, x
                        torch.cuda.empty_cache() if torch.cuda.is_available() else None
                        
                    except Exception as e:
                        print(f"  âš ï¸ {key} failed: {e}")
                        continue
        
        self.results['ssm_variants'] = ssm_results
        print("âœ… SSM Variants Benchmark Completed")
    
    def benchmark_memory_scaling(self):
        """ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ë§ í…ŒìŠ¤íŠ¸"""
        print("ğŸ§ª Memory Scaling Benchmark Started...")
        
        if not torch.cuda.is_available():
            print("âš ï¸ CUDA not available, skipping memory benchmark")
            return
        
        memory_results = {}
        
        # Flow Matching ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ë§
        dims = [1024, 2048, 4096, 8192, 16384]
        batch_size = 4
        
        for dim in dims:
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated()
            
            try:
                flow = RectifiedFlow(dim=dim, condition_dim=768).cuda()
                condition = torch.randn(batch_size, 768, device='cuda')
                x1 = torch.randn(batch_size, dim, device='cuda')
                
                # ìˆœì „íŒŒ
                loss = flow.compute_loss(x1, condition)
                current_memory = torch.cuda.memory_allocated()
                
                memory_results[f'flow_dim_{dim}'] = {
                    'memory_mb': (current_memory - initial_memory) / 1024 / 1024,
                    'dim': dim,
                    'type': 'flow_matching'
                }
                
                del flow, condition, x1, loss
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    memory_results[f'flow_dim_{dim}'] = {
                        'memory_mb': float('inf'),
                        'dim': dim,
                        'type': 'flow_matching',
                        'oom': True
                    }
                    break
        
        # SSM ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ë§
        seq_lens = [100, 500, 1000, 2000, 4000]
        d_model = 768
        
        for seq_len in seq_lens:
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated()
            
            try:
                encoder = create_adaptive_ssm_encoder(d_model=d_model, target_speed='fast').cuda()
                x = torch.randn(batch_size, seq_len, d_model, device='cuda')
                
                # ìˆœì „íŒŒ
                output = encoder(x)
                current_memory = torch.cuda.memory_allocated()
                
                memory_results[f'ssm_seq_{seq_len}'] = {
                    'memory_mb': (current_memory - initial_memory) / 1024 / 1024,
                    'seq_len': seq_len,
                    'type': 'ssm'
                }
                
                del encoder, x, output
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    memory_results[f'ssm_seq_{seq_len}'] = {
                        'memory_mb': float('inf'),
                        'seq_len': seq_len,
                        'type': 'ssm',
                        'oom': True
                    }
                    break
        
        self.results['memory_scaling'] = memory_results
        print("âœ… Memory Scaling Benchmark Completed")
    
    def generate_report(self):
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        print("\nğŸ“Š Performance Benchmark Report")
        print("=" * 50)
        
        # Flow Matching ê²°ê³¼
        if 'flow_matching' in self.results:
            print("\nğŸ”¥ Flow Matching Performance:")
            flow_results = self.results['flow_matching']
            
            for key, result in flow_results.items():
                print(f"  {key}:")
                print(f"    Loss computation: {result['loss_time_avg']*1000:.2f}Â±{result['loss_time_std']*1000:.2f}ms")
                print(f"    Sampling (4 steps): {result['sample_time_avg']*1000:.2f}Â±{result['sample_time_std']*1000:.2f}ms")
                print(f"    Memory usage: {result['memory_mb']:.2f}MB")
        
        # SSM ê²°ê³¼
        if 'ssm_variants' in self.results:
            print("\nğŸš€ SSM Variants Performance:")
            ssm_results = self.results['ssm_variants']
            
            # ë¸”ë¡ë³„ ìš”ì•½
            block_summary = {}
            for key, result in ssm_results.items():
                block_type = result['block_type']
                if block_type not in block_summary:
                    block_summary[block_type] = []
                block_summary[block_type].append(result['tokens_per_sec'])
            
            for block_type, throughputs in block_summary.items():
                avg_throughput = np.mean(throughputs)
                print(f"  {block_type}: {avg_throughput:.0f} tokens/sec (avg)")
        
        # ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ë§ ê²°ê³¼
        if 'memory_scaling' in self.results:
            print("\nğŸ’¾ Memory Scaling:")
            memory_results = self.results['memory_scaling']
            
            print("  Flow Matching:")
            for key, result in memory_results.items():
                if result['type'] == 'flow_matching':
                    if result.get('oom'):
                        print(f"    dim {result['dim']}: OOM")
                    else:
                        print(f"    dim {result['dim']}: {result['memory_mb']:.2f}MB")
            
            print("  SSM Encoder:")
            for key, result in memory_results.items():
                if result['type'] == 'ssm':
                    if result.get('oom'):
                        print(f"    seq_len {result['seq_len']}: OOM")
                    else:
                        print(f"    seq_len {result['seq_len']}: {result['memory_mb']:.2f}MB")
    
    def save_results(self, filename="benchmark_results.txt"):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("ğŸš€ Lymic Performance Benchmark Results\n")
            f.write(f"Device: {self.device}\n")
            f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 50 + "\n\n")
            
            # ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì €ì¥
            import json
            for category, results in self.results.items():
                f.write(f"\n{category.upper()}:\n")
                for key, result in results.items():
                    f.write(f"  {key}: {json.dumps(result, indent=4)}\n")
        
        print(f"ğŸ“„ Results saved to {filename}")

def quick_benchmark():
    """ë¹ ë¥¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    benchmark = PerformanceBenchmark()
    
    print("ğŸš€ Quick Benchmark Starting...")
    
    # ì‘ì€ í¬ê¸°ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
    benchmark.benchmark_flow_matching(
        dims=[1024, 4096], 
        batch_sizes=[2, 4], 
        num_runs=5
    )
    
    benchmark.benchmark_ssm_variants(
        d_models=[256, 512], 
        seq_lens=[100, 500], 
        num_runs=5
    )
    
    benchmark.benchmark_memory_scaling()
    
    benchmark.generate_report()
    benchmark.save_results("quick_benchmark.txt")
    
    return benchmark

if __name__ == "__main__":
    # ëª…ë ¹í–‰ ì‹¤í–‰
    import argparse
    
    parser = argparse.ArgumentParser(description='Lymic Performance Benchmark')
    parser.add_argument('--quick', action='store_true', help='Run quick benchmark')
    parser.add_argument('--full', action='store_true', help='Run full benchmark')
    
    args = parser.parse_args()
    
    if args.quick or (not args.full):
        quick_benchmark()
    elif args.full:
        benchmark = PerformanceBenchmark()
        
        print("ğŸš€ Full Benchmark Starting (this may take a while)...")
        
        benchmark.benchmark_flow_matching()
        benchmark.benchmark_ssm_variants()
        benchmark.benchmark_memory_scaling()
        
        benchmark.generate_report()
        benchmark.save_results("full_benchmark.txt")
