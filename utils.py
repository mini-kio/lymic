import torch
import torch.nn as nn
import torchaudio
import numpy as np
import librosa
from pathlib import Path
from torch.utils.data import Dataset
import json
import random
import shutil
import warnings
from functools import lru_cache
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor
import time

warnings.filterwarnings("ignore", category=UserWarning)

@lru_cache(maxsize=1000)
def extract_f0_cached(audio_hash, sample_rate=44100, hop_length=512, f0_min=80, f0_max=800, method='pyin'):
    """
    ğŸš€ ìºì‹œëœ F0 ì¶”ì¶œ - ë™ì¼í•œ ì˜¤ë””ì˜¤ì— ëŒ€í•´ ì¬ê³„ì‚° ë°©ì§€
    """
    # ì‹¤ì œë¡œëŠ” audioë¥¼ ë°›ì•„ì•¼ í•˜ì§€ë§Œ, ìºì‹±ì„ ìœ„í•´ í•´ì‹œ ì‚¬ìš©
    # ì´ í•¨ìˆ˜ëŠ” ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì•„ë˜ì˜ extract_f0ë¥¼ ì‚¬ìš©
    pass

def extract_f0(audio, sample_rate=44100, hop_length=512, f0_min=80, f0_max=800, method='pyin'):
    """
    ğŸ”¥ ìµœì í™”ëœ F0 ì¶”ì¶œ
    - ë” ë¹ ë¥¸ íŒŒë¼ë¯¸í„°
    - ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
    """
    if isinstance(audio, torch.Tensor):
        audio = audio.cpu().numpy()
    
    # ì…ë ¥ ê²€ì¦
    if len(audio) == 0:
        return np.array([]), np.array([])
    
    try:
        if method == 'pyin':
            # ğŸ”¥ ìµœì í™”ëœ pyin íŒŒë¼ë¯¸í„°
            f0, voiced_flag, voiced_probs = librosa.pyin(
                audio,
                fmin=f0_min,
                fmax=f0_max,
                sr=sample_rate,
                hop_length=hop_length,
                frame_length=hop_length * 3,  # ë” ë¹ ë¥´ê²Œ
                win_length=hop_length * 2,    # ë” ë¹ ë¥´ê²Œ
                resolution=0.1,               # ë” ë¹ ë¥´ê²Œ
                max_transition_rate=35.92,    # ê¸°ë³¸ê°’
                switch_prob=0.01,             # ë” ë¹ ë¥´ê²Œ
                no_trough_prob=0.01           # ë” ë¹ ë¥´ê²Œ
            )
            
            # NaN ì²˜ë¦¬
            valid_mask = ~np.isnan(f0)
            f0 = np.nan_to_num(f0, nan=0.0)
            
            # VUV í”Œë˜ê·¸ ìƒì„±
            vuv = valid_mask & (f0 > f0_min/2)  # ë” ê´€ëŒ€í•œ ì„ê³„ê°’
            vuv = vuv.astype(np.float32)
            
        else:
            raise ValueError(f"Unsupported F0 extraction method: {method}")
        
        # ê¸¸ì´ ê²€ì¦
        expected_frames = 1 + len(audio) // hop_length
        if len(f0) != expected_frames:
            # ê¸¸ì´ ì¡°ì •
            if len(f0) > expected_frames:
                f0 = f0[:expected_frames]
                vuv = vuv[:expected_frames]
            else:
                # íŒ¨ë”©
                pad_length = expected_frames - len(f0)
                f0 = np.pad(f0, (0, pad_length), mode='constant', constant_values=0)
                vuv = np.pad(vuv, (0, pad_length), mode='constant', constant_values=0)
        
        return f0.astype(np.float32), vuv.astype(np.float32)
        
    except Exception as e:
        print(f"âš ï¸ F0 extraction failed: {e}")
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê¸¸ì´ë¡œ 0 ë°˜í™˜
        expected_frames = 1 + len(audio) // hop_length
        return np.zeros(expected_frames, dtype=np.float32), np.zeros(expected_frames, dtype=np.float32)

def compute_vuv(f0, threshold=50.0):  # ë” ë†’ì€ ì„ê³„ê°’
    """VUV ê³„ì‚° - ìµœì í™”ë¨"""
    if isinstance(f0, torch.Tensor):
        return (f0 > threshold).float()
    else:
        return (f0 > threshold).astype(np.float32)

def normalize_f0(f0, method='log', f0_min=50, f0_max=1000):  # ë” ë„“ì€ ë²”ìœ„
    """
    ğŸ”¥ ìµœì í™”ëœ F0 ì •ê·œí™”
    - ë” ì•ˆì •ì ì¸ ì •ê·œí™”
    - ê·¹ê°’ ì²˜ë¦¬ ê°œì„ 
    """
    if isinstance(f0, torch.Tensor):
        is_torch = True
        device = f0.device
        f0_np = f0.cpu().numpy()
    else:
        is_torch = False
        f0_np = f0.copy()
    
    # Voiced ë§ˆìŠ¤í¬
    voiced = f0_np > 0
    
    if not np.any(voiced):
        # ëª¨ë“  í”„ë ˆì„ì´ unvoicedì¸ ê²½ìš°
        result = np.zeros_like(f0_np)
    else:
        f0_norm = np.zeros_like(f0_np)
        
        if method == 'log':
            # ë¡œê·¸ ì •ê·œí™” (ë” ì•ˆì •ì )
            f0_voiced = f0_np[voiced]
            f0_voiced = np.clip(f0_voiced, f0_min, f0_max)
            
            # ë¡œê·¸ ë³€í™˜
            log_f0 = np.log(f0_voiced + 1e-8)  # ìˆ˜ì¹˜ ì•ˆì •ì„±
            log_f0_min, log_f0_max = np.log(f0_min), np.log(f0_max)
            
            # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
            f0_norm[voiced] = 2 * (log_f0 - log_f0_min) / (log_f0_max - log_f0_min) - 1
            
        elif method == 'linear':
            # ì„ í˜• ì •ê·œí™”
            f0_voiced = f0_np[voiced]
            f0_voiced = np.clip(f0_voiced, f0_min, f0_max)
            f0_norm[voiced] = 2 * (f0_voiced - f0_min) / (f0_max - f0_min) - 1
            
        else:
            raise ValueError(f"Unknown normalization method: {method}")
        
        result = f0_norm
    
    if is_torch:
        return torch.from_numpy(result).to(device).float()
    else:
        return result.astype(np.float32)

def denormalize_f0(f0_norm, method='log', f0_min=50, f0_max=1000):
    """F0 ì—­ì •ê·œí™”"""
    if isinstance(f0_norm, torch.Tensor):
        is_torch = True
        device = f0_norm.device
        f0_np = f0_norm.cpu().numpy()
    else:
        is_torch = False
        f0_np = f0_norm.copy()
    
    if method == 'log':
        log_f0_min, log_f0_max = np.log(f0_min), np.log(f0_max)
        log_f0 = (f0_np + 1) / 2 * (log_f0_max - log_f0_min) + log_f0_min
        f0 = np.exp(log_f0) - 1e-8
    elif method == 'linear':
        f0 = (f0_np + 1) / 2 * (f0_max - f0_min) + f0_min
    else:
        raise ValueError(f"Unknown normalization method: {method}")
    
    if is_torch:
        return torch.from_numpy(f0).to(device).float()
    else:
        return f0.astype(np.float32)

class OptimizedVoiceConversionDataset(Dataset):
    """
    ğŸš€ ìµœì í™”ëœ Voice Conversion Dataset
    - ë©€í‹°í”„ë¡œì„¸ì‹± F0 ì¶”ì¶œ
    - ìºì‹± ì‹œìŠ¤í…œ
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”©
    - ë” ë¹ ë¥¸ ë°ì´í„° ì²˜ë¦¬
    """
    
    def __init__(self, data_dir, sample_rate=44100, waveform_length=16384, 
                 channels=2, min_files_per_speaker=5, extract_f0=True, 
                 hop_length=512, f0_method='pyin', use_cache=True, 
                 max_workers=None):
        
        self.data_dir = Path(data_dir)
        self.sample_rate = sample_rate
        self.waveform_length = waveform_length
        self.channels = channels
        self.min_files_per_speaker = min_files_per_speaker
        self.extract_f0 = extract_f0
        self.hop_length = hop_length
        self.f0_method = f0_method
        self.use_cache = use_cache
        
        # ğŸ”¥ ìºì‹œ ë””ë ‰í† ë¦¬
        self.cache_dir = self.data_dir / '.cache'
        if self.use_cache:
            self.cache_dir.mkdir(exist_ok=True)
        
        # ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì •
        self.max_workers = max_workers or min(8, mp.cpu_count())
        
        print(f"ğŸš€ Initializing optimized dataset:")
        print(f"   Cache: {'âœ… Enabled' if self.use_cache else 'âŒ Disabled'}")
        print(f"   F0 extraction: {'âœ… Enabled' if self.extract_f0 else 'âŒ Disabled'}")
        print(f"   Max workers: {self.max_workers}")
        
        # ë°ì´í„° ìŠ¤ìº”
        self._scan_dataset()
        
        # F0 ìºì‹œ ì¤€ë¹„
        if self.extract_f0:
            self._prepare_f0_cache()
    
    def _scan_dataset(self):
        """ğŸ”¥ ìµœì í™”ëœ ë°ì´í„°ì…‹ ìŠ¤ìº”"""
        start_time = time.time()
        
        self.speakers = []
        self.speaker_files = {}
        
        # ë³‘ë ¬ë¡œ í™”ì í´ë” ìŠ¤ìº”
        speaker_dirs = [d for d in self.data_dir.iterdir() if d.is_dir() and not d.name.startswith('.')]
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            results = list(executor.map(self._scan_speaker_dir, speaker_dirs))
        
        # ê²°ê³¼ ì§‘ê³„
        for speaker_name, audio_files in results:
            if speaker_name and len(audio_files) >= self.min_files_per_speaker:
                self.speakers.append(speaker_name)
                self.speaker_files[speaker_name] = sorted(audio_files)
                print(f"ğŸ“ {speaker_name}: {len(audio_files)} files")
            elif speaker_name:
                print(f"âš ï¸ {speaker_name}: {len(audio_files)} files (< {self.min_files_per_speaker}, skipped)")
        
        # í™”ì ID ë§¤í•‘
        self.speaker_to_id = {spk: i for i, spk in enumerate(sorted(self.speakers))}
        self.id_to_speaker = {i: spk for spk, i in self.speaker_to_id.items()}
        
        # í›ˆë ¨ í˜ì–´ ìƒì„±
        self._generate_training_pairs()
        
        scan_time = time.time() - start_time
        print(f"âš¡ Dataset scan completed in {scan_time:.2f}s")
    
    def _scan_speaker_dir(self, speaker_dir):
        """ê°œë³„ í™”ì ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
        speaker_name = speaker_dir.name
        audio_files = []
        
        for ext in ['*.wav', '*.mp3', '*.flac', '*.m4a']:
            audio_files.extend(list(speaker_dir.glob(ext)))
        
        return speaker_name, audio_files
    
    def _generate_training_pairs(self):
        """í›ˆë ¨ í˜ì–´ ìƒì„± - ìµœì í™”ë¨"""
        self.training_pairs = []
        
        for source_speaker in self.speakers:
            source_files = self.speaker_files[source_speaker]
            
            for target_speaker in self.speakers:
                if target_speaker != source_speaker:
                    target_speaker_id = self.speaker_to_id[target_speaker]
                    
                    for source_file in source_files:
                        self.training_pairs.append({
                            'source_file': source_file,
                            'source_speaker': source_speaker,
                            'target_speaker': target_speaker,
                            'target_speaker_id': target_speaker_id
                        })
        
        print(f"ğŸ¯ Generated {len(self.training_pairs)} training pairs")
    
    def _prepare_f0_cache(self):
        """F0 ìºì‹œ ì¤€ë¹„"""
        if not self.use_cache:
            return
        
        cache_info_file = self.cache_dir / 'f0_cache_info.json'
        
        if cache_info_file.exists():
            with open(cache_info_file, 'r') as f:
                cache_info = json.load(f)
            
            # ìºì‹œ ìœ íš¨ì„± ê²€ì¦
            if (cache_info.get('sample_rate') == self.sample_rate and
                cache_info.get('hop_length') == self.hop_length and
                cache_info.get('f0_method') == self.f0_method):
                print("âœ… F0 cache is valid")
                return
        
        print("ğŸ”„ Building F0 cache...")
        self._build_f0_cache()
    
    def _build_f0_cache(self):
        """F0 ìºì‹œ êµ¬ì¶•"""
        all_files = []
        for files in self.speaker_files.values():
            all_files.extend(files)
        
        print(f"ğŸ”„ Processing {len(all_files)} files for F0 cache...")
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ F0 ì¶”ì¶œ
        batch_size = 50
        for i in range(0, len(all_files), batch_size):
            batch_files = all_files[i:i+batch_size]
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                executor.map(self._extract_and_cache_f0, batch_files)
            
            print(f"Progress: {min(i+batch_size, len(all_files))}/{len(all_files)}")
        
        # ìºì‹œ ì •ë³´ ì €ì¥
        cache_info = {
            'sample_rate': self.sample_rate,
            'hop_length': self.hop_length,
            'f0_method': self.f0_method,
            'total_files': len(all_files)
        }
        
        with open(self.cache_dir / 'f0_cache_info.json', 'w') as f:
            json.dump(cache_info, f)
        
        print("âœ… F0 cache built successfully")
    
    def _extract_and_cache_f0(self, audio_file):
        """ê°œë³„ íŒŒì¼ì˜ F0 ì¶”ì¶œ ë° ìºì‹œ"""
        cache_file = self.cache_dir / f"{audio_file.stem}_f0.npz"
        
        if cache_file.exists():
            return  # ì´ë¯¸ ìºì‹œë¨
        
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ (ì§§ê²Œ)
            waveform, sr = torchaudio.load(audio_file)
            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
                waveform = resampler(waveform)
            
            # ëª¨ë…¸ ë³€í™˜
            if waveform.size(0) > 1:
                waveform = waveform.mean(dim=0)
            else:
                waveform = waveform.squeeze(0)
            
            # F0 ì¶”ì¶œ
            f0, vuv = extract_f0(
                waveform.numpy(),
                sample_rate=self.sample_rate,
                hop_length=self.hop_length,
                method=self.f0_method
            )
            
            # ì •ê·œí™”
            f0_normalized = normalize_f0(f0, method='log')
            
            # ìºì‹œ ì €ì¥
            np.savez_compressed(
                cache_file,
                f0=f0,
                f0_normalized=f0_normalized,
                vuv=vuv
            )
            
        except Exception as e:
            print(f"âš ï¸ Failed to cache F0 for {audio_file}: {e}")
    
    def _load_cached_f0(self, audio_file):
        """ìºì‹œëœ F0 ë¡œë“œ"""
        if not self.use_cache:
            return None
        
        cache_file = self.cache_dir / f"{audio_file.stem}_f0.npz"
        
        if cache_file.exists():
            try:
                data = np.load(cache_file)
                return {
                    'f0': data['f0'],
                    'f0_normalized': data['f0_normalized'],
                    'vuv': data['vuv']
                }
            except Exception as e:
                print(f"âš ï¸ Failed to load cached F0 for {audio_file}: {e}")
        
        return None
    
    def __len__(self):
        return len(self.training_pairs)
    
    def __getitem__(self, idx):
        pair = self.training_pairs[idx]
        
        # ğŸš€ ì˜¤ë””ì˜¤ ë¡œë”©
        source_waveform = self._load_audio(pair['source_file'])
        
        # íƒ€ê²Ÿ ì˜¤ë””ì˜¤ (ëœë¤ ì„ íƒ)
        target_files = self.speaker_files[pair['target_speaker']]
        target_file = random.choice(target_files)
        target_waveform = self._load_audio(target_file)
        
        result = {
            'source_waveform': source_waveform,
            'target_waveform': target_waveform,
            'target_speaker_id': torch.tensor(pair['target_speaker_id'], dtype=torch.long),
            'source_speaker': pair['source_speaker'],
            'target_speaker': pair['target_speaker'],
            'source_file': str(pair['source_file']),
            'target_file': str(target_file)
        }
        
        # ğŸµ F0 ì²˜ë¦¬
        if self.extract_f0:
            # ìºì‹œì—ì„œ ì‹œë„
            f0_data = self._load_cached_f0(target_file)
            
            if f0_data is not None:
                # ìºì‹œëœ ë°ì´í„° ì‚¬ìš©
                f0_normalized = f0_data['f0_normalized']
                vuv = f0_data['vuv']
            else:
                # ì‹¤ì‹œê°„ ì¶”ì¶œ
                if target_waveform.dim() == 2:
                    target_mono = target_waveform.mean(dim=0)
                else:
                    target_mono = target_waveform
                
                try:
                    f0, vuv = extract_f0(
                        target_mono.numpy(),
                        sample_rate=self.sample_rate,
                        hop_length=self.hop_length,
                        method=self.f0_method
                    )
                    f0_normalized = normalize_f0(f0, method='log')
                except Exception as e:
                    print(f"âš ï¸ F0 extraction failed: {e}")
                    # ê¸°ë³¸ê°’ ì‚¬ìš©
                    default_frames = self.waveform_length // self.hop_length + 1
                    f0_normalized = np.zeros(default_frames, dtype=np.float32)
                    vuv = np.zeros(default_frames, dtype=np.float32)
            
            result['f0_target'] = torch.from_numpy(f0_normalized).float()
            result['vuv_target'] = torch.from_numpy(vuv).float()
        
        return result
    
    def _load_audio(self, file_path):
        """ğŸš€ ìµœì í™”ëœ ì˜¤ë””ì˜¤ ë¡œë”©"""
        try:
            # ğŸ”¥ torchaudioë¡œ ë¹ ë¥¸ ë¡œë”©
            waveform, sr = torchaudio.load(str(file_path))
            
            # ë¦¬ìƒ˜í”Œë§ (í•„ìš”ì‹œ)
            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
                waveform = resampler(waveform)
            
            # ì±„ë„ ì²˜ë¦¬
            if self.channels == 2:
                if waveform.size(0) == 1:
                    waveform = waveform.repeat(2, 1)
                elif waveform.size(0) > 2:
                    waveform = waveform[:2]
            else:
                if waveform.size(0) > 1:
                    waveform = waveform.mean(dim=0, keepdim=True)
            
            # ê¸¸ì´ ì¡°ì •
            if self.channels == 2:
                waveform = self._fix_length(waveform, self.waveform_length, dim=1)
            else:
                waveform = self._fix_length(waveform.squeeze(0), self.waveform_length, dim=0)
            
            return waveform
            
        except Exception as e:
            print(f"âš ï¸ Failed to load audio {file_path}: {e}")
            # ê¸°ë³¸ ë…¸ì´ì¦ˆ ë°˜í™˜
            if self.channels == 2:
                return torch.randn(2, self.waveform_length) * 0.01
            else:
                return torch.randn(self.waveform_length) * 0.01
    
    def _fix_length(self, waveform, target_length, dim=0):
        """ê¸¸ì´ ì¡°ì • - ìµœì í™”ë¨"""
        current_length = waveform.size(dim)
        
        if current_length > target_length:
            # ëœë¤ í¬ë¡­ (ë” íš¨ìœ¨ì )
            start = torch.randint(0, current_length - target_length + 1, (1,)).item()
            if dim == 0:
                return waveform[start:start + target_length]
            else:
                return waveform[:, start:start + target_length]
        elif current_length < target_length:
            # ì œë¡œ íŒ¨ë”©
            pad_length = target_length - current_length
            if dim == 0:
                return torch.cat([waveform, torch.zeros(pad_length)], dim=0)
            else:
                return torch.cat([waveform, torch.zeros(waveform.size(0), pad_length)], dim=1)
        else:
            return waveform
    
    def get_speaker_info(self):
        """í™”ì ì •ë³´ ë°˜í™˜"""
        return {
            'speakers': self.speakers,
            'speaker_to_id': self.speaker_to_id,
            'id_to_speaker': self.id_to_speaker,
            'total_speakers': len(self.speakers),
            'total_pairs': len(self.training_pairs),
            'files_per_speaker': {spk: len(files) for spk, files in self.speaker_files.items()}
        }
    
    def print_sample_pairs(self, num_samples=5):
        """ìƒ˜í”Œ í˜ì–´ ì¶œë ¥"""
        print(f"\nğŸ” Sample training pairs:")
        for i in range(min(num_samples, len(self.training_pairs))):
            pair = self.training_pairs[i]
            print(f"   {i+1}. {pair['source_speaker']} â†’ {pair['target_speaker']}")
            print(f"      ğŸ“ {Path(pair['source_file']).name}")
        
        if self.extract_f0:
            print(f"   ğŸµ F0 conditioning enabled with cache")
        print()

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
VoiceConversionDataset = OptimizedVoiceConversionDataset

def optimized_collate_fn(batch):
    """ğŸš€ ìµœì í™”ëœ collate í•¨ìˆ˜"""
    # ì²« ë²ˆì§¸ ì•„ì´í…œì—ì„œ ì°¨ì› í™•ì¸
    first_source = batch[0]['source_waveform']
    is_stereo = first_source.dim() == 2
    batch_size = len(batch)
    
    # íš¨ìœ¨ì ì¸ ìŠ¤íƒí‚¹
    if is_stereo:
        source_waveforms = torch.stack([item['source_waveform'] for item in batch])
        target_waveforms = torch.stack([item['target_waveform'] for item in batch])
    else:
        source_waveforms = torch.stack([item['source_waveform'] for item in batch])
        target_waveforms = torch.stack([item['target_waveform'] for item in batch])
    
    target_speaker_ids = torch.stack([item['target_speaker_id'] for item in batch])
    
    result = {
        'source_waveform': source_waveforms,
        'target_waveform': target_waveforms,
        'target_speaker_id': target_speaker_ids,
        'source_speakers': [item['source_speaker'] for item in batch],
        'target_speakers': [item['target_speaker'] for item in batch]
    }
    
    # ğŸµ F0/VUV ì²˜ë¦¬ (ìµœì í™”ë¨)
    if 'f0_target' in batch[0]:
        # ë°°ì¹˜ ë‚´ ìµœëŒ€ ê¸¸ì´ ì°¾ê¸°
        max_f0_len = max(item['f0_target'].size(0) for item in batch)
        
        # ë¯¸ë¦¬ í• ë‹¹
        f0_targets = torch.zeros(batch_size, max_f0_len)
        vuv_targets = torch.zeros(batch_size, max_f0_len)
        
        for i, item in enumerate(batch):
            f0 = item['f0_target']
            vuv = item['vuv_target']
            
            # ê¸¸ì´ ì¡°ì •
            actual_len = min(f0.size(0), max_f0_len)
            f0_targets[i, :actual_len] = f0[:actual_len]
            vuv_targets[i, :actual_len] = vuv[:actual_len]
        
        result['f0_target'] = f0_targets
        result['vuv_target'] = vuv_targets
    
    return result

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
collate_fn = optimized_collate_fn

# ğŸš€ ì¶”ê°€ ìµœì í™” ìœ í‹¸ë¦¬í‹°ë“¤

def create_optimized_train_val_split(dataset_root, train_ratio=0.8, use_symlinks=True):
    """
    ìµœì í™”ëœ train/val ë¶„í• 
    - ì‹¬ë³¼ë¦­ ë§í¬ ì‚¬ìš©ìœ¼ë¡œ ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½
    - ë³‘ë ¬ ì²˜ë¦¬
    """
    dataset_root = Path(dataset_root)
    train_dir = dataset_root / 'train'
    val_dir = dataset_root / 'val'
    
    # ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì •ë¦¬
    if train_dir.exists():
        shutil.rmtree(train_dir)
    if val_dir.exists():
        shutil.rmtree(val_dir)
    
    train_dir.mkdir()
    val_dir.mkdir()
    
    def process_speaker(speaker_dir):
        if not speaker_dir.is_dir() or speaker_dir.name in ['train', 'val', '.cache']:
            return None, 0, 0
        
        speaker_name = speaker_dir.name
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ìˆ˜ì§‘
        audio_files = []
        for ext in ['*.wav', '*.mp3', '*.flac', '*.m4a']:
            audio_files.extend(list(speaker_dir.glob(ext)))
        
        if len(audio_files) < 2:
            return speaker_name, 0, 0
        
        # ë¶„í• 
        random.shuffle(audio_files)
        split_idx = int(len(audio_files) * train_ratio)
        
        train_files = audio_files[:split_idx]
        val_files = audio_files[split_idx:]
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        (train_dir / speaker_name).mkdir()
        (val_dir / speaker_name).mkdir()
        
        # íŒŒì¼ ë³µì‚¬ ë˜ëŠ” ë§í¬
        for file in train_files:
            dest = train_dir / speaker_name / file.name
            if use_symlinks:
                dest.symlink_to(file.absolute())
            else:
                shutil.copy2(file, dest)
        
        for file in val_files:
            dest = val_dir / speaker_name / file.name
            if use_symlinks:
                dest.symlink_to(file.absolute())
            else:
                shutil.copy2(file, dest)
        
        return speaker_name, len(train_files), len(val_files)
    
    # ë³‘ë ¬ ì²˜ë¦¬
    speaker_dirs = [d for d in dataset_root.iterdir() if d.is_dir()]
    
    with ThreadPoolExecutor(max_workers=min(8, mp.cpu_count())) as executor:
        results = list(executor.map(process_speaker, speaker_dirs))
    
    # ê²°ê³¼ ì¶œë ¥
    total_train = 0
    total_val = 0
    
    for speaker_name, train_count, val_count in results:
        if speaker_name and (train_count > 0 or val_count > 0):
            print(f"ğŸ“ {speaker_name}: {train_count} train, {val_count} val")
            total_train += train_count
            total_val += val_count
    
    print(f"âœ… Optimized Train/Val split completed!")
    print(f"   ğŸ“ Total train files: {total_train}")
    print(f"   ğŸ“ Total val files: {total_val}")
    print(f"   ğŸ”— Using {'symlinks' if use_symlinks else 'copies'}")

def benchmark_dataset_loading(dataset, num_samples=100):
    """ë°ì´í„°ì…‹ ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    print(f"ğŸš€ Benchmarking dataset loading ({num_samples} samples)...")
    
    start_time = time.time()
    
    for i in range(min(num_samples, len(dataset))):
        sample = dataset[i]
    
    end_time = time.time()
    total_time = end_time - start_time
    avg_time = total_time / num_samples
    
    print(f"ğŸ“Š Loading benchmark results:")
    print(f"   Total time: {total_time:.2f}s")
    print(f"   Average per sample: {avg_time*1000:.2f}ms")
    print(f"   Samples per second: {num_samples/total_time:.1f}")
    
    return avg_time