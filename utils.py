import torch
import torch.nn as nn
import torchaudio
import numpy as np
import librosa
from pathlib import Path
from torch.utils.data import Dataset
import json
import random
import shutil
import warnings
from functools import lru_cache
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor
import time
from tqdm import tqdm

try:
    import crepe
    CREPE_AVAILABLE = True
except ImportError:
    CREPE_AVAILABLE = False
    print("Warning: CREPE not available. Install with: pip install crepe tensorflow")

try:
    from scipy import interpolate
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

warnings.filterwarnings("ignore", category=UserWarning)

@lru_cache(maxsize=1000)
def extract_f0_cached(audio_hash, sample_rate=44100, hop_length=512, f0_min=80, f0_max=800, method='pyin'):
    """
    ìºì‹œëœ F0 ì¶”ì¶œ - ë™ì¼í•œ ì˜¤ë””ì˜¤ì— ëŒ€í•´ ì¬ê³„ì‚° ë°©ì§€
    """
    # ì‹¤ì œë¡œëŠ” audioë¥¼ ë°›ì•„ì•¼ í•˜ì§€ë§Œ, ìºì‹±ì„ ìœ„í•´ í•´ì‹œ ì‚¬ìš©
    # ì´ í•¨ìˆ˜ëŠ” ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì•„ë˜ì˜ extract_f0ë¥¼ ì‚¬ìš©
    pass

def extract_f0_crepe(audio, sample_rate=44100, hop_length=512, model_capacity='small'):
    """
    CREPEë¡œ ì •í™•í•˜ê³  ë¹ ë¥¸ F0 ì¶”ì¶œ
    
    Args:
        model_capacity: 'tiny', 'small', 'medium', 'large', 'full'
                       tiny: ê°€ì¥ ë¹ ë¦„ (3ë°°), ì•½ê°„ ì •í™•ë„ í•˜ë½
                       small: ê· í˜•ì¡íŒ ì„ íƒ (2ë°° ë¹ ë¦„, ë†’ì€ ì •í™•ë„)
                       full: ìµœê³  ì •í™•ë„, ê°€ì¥ ëŠë¦¼
    """
    if not CREPE_AVAILABLE:
        raise ImportError("CREPE not available. Install with: pip install crepe tensorflow")
    
    if isinstance(audio, torch.Tensor):
        audio = audio.cpu().numpy()
    
    # CREPEëŠ” 16kHzë¡œ í›ˆë ¨ë¨ (ë‚´ë¶€ì—ì„œ ë¦¬ìƒ˜í”Œë§)
    time, frequency, confidence, activation = crepe.predict(
        audio,
        sample_rate,
        model_capacity=model_capacity,  # 'small'ì´ ì†ë„-ì •í™•ë„ ê· í˜•ì 
        step_size=hop_length / sample_rate * 1000,  # ms ë‹¨ìœ„
        viterbi=True,  # ìŠ¤ë¬´ë”© ì ìš©
        center=True,
        verbose=0
    )
    
    # Confidence ê¸°ë°˜ VUV (0.5 ì„ê³„ê°’)
    vuv = (confidence > 0.5).astype(np.float32)
    f0 = frequency * vuv  # Unvoiced êµ¬ê°„ì€ 0ìœ¼ë¡œ
    
    # hop_lengthì— ë§ì¶° ê¸¸ì´ ì¡°ì •
    expected_frames = 1 + len(audio) // hop_length
    if len(f0) != expected_frames and SCIPY_AVAILABLE:
        # ì„ í˜• ë³´ê°„ìœ¼ë¡œ ê¸¸ì´ ë§ì¶¤
        old_indices = np.linspace(0, len(f0) - 1, len(f0))
        new_indices = np.linspace(0, len(f0) - 1, expected_frames)
        
        f_interp = interpolate.interp1d(old_indices, f0, kind='linear', 
                                       bounds_error=False, fill_value=0)
        v_interp = interpolate.interp1d(old_indices, vuv, kind='linear', 
                                       bounds_error=False, fill_value=0)
        
        f0 = f_interp(new_indices)
        vuv = v_interp(new_indices)
        vuv = (vuv > 0.5).astype(np.float32)  # Re-threshold
    
    return f0.astype(np.float32), vuv.astype(np.float32)

def extract_f0_fast_pyin(audio, sample_rate=44100, hop_length=512, f0_min=80, f0_max=800):
    """ìµœì í™”ëœ ë¹ ë¥¸ pYIN (ë°±ì—…ìš©)"""
    f0, voiced_flag, _ = librosa.pyin(
        audio,
        fmin=f0_min, fmax=f0_max, sr=sample_rate, hop_length=hop_length,
        frame_length=hop_length * 2,  # 3â†’2
        win_length=hop_length,        # 2â†’1
        resolution=0.2,               # 0.1â†’0.2  
        n_thresholds=20,              # 100â†’20 (5ë°° ë¹ ë¦„!)
        max_transition_rate=50,
        switch_prob=0.02,
        no_trough_prob=0.02
    )
    
    f0 = np.nan_to_num(f0, nan=0.0)
    vuv = voiced_flag.astype(np.float32)
    
    return f0, vuv

def extract_f0_hybrid(audio, sample_rate=44100, hop_length=512):
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë°©ë²•: CREPE + pYIN ê²°í•©ìœ¼ë¡œ ìµœê³  ì •í™•ë„
    """
    if not CREPE_AVAILABLE:
        return extract_f0_fast_pyin(audio, sample_rate, hop_length)
    
    # CREPE (ì£¼ìš”)
    f0_crepe, vuv_crepe = extract_f0_crepe(
        audio, sample_rate, hop_length, model_capacity='small'
    )
    
    # pYIN (ë°±ì—… ë° ê²€ì¦)
    f0_pyin, vuv_pyin = extract_f0_fast_pyin(
        audio, sample_rate, hop_length
    )
    
    # CREPE confidenceê°€ ë‚®ì€ êµ¬ê°„ì—ì„œ pYIN ì‚¬ìš©
    try:
        _, _, confidence, _ = crepe.predict(
            audio, sample_rate, model_capacity='small',
            step_size=hop_length / sample_rate * 1000, verbose=0
        )
        
        # ê¸¸ì´ ë§ì¶¤
        if len(confidence) != len(f0_crepe) and SCIPY_AVAILABLE:
            old_idx = np.linspace(0, len(confidence)-1, len(confidence))
            new_idx = np.linspace(0, len(confidence)-1, len(f0_crepe))
            confidence = interpolate.interp1d(old_idx, confidence)(new_idx)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²°í•©
        confidence_threshold = 0.3
        low_confidence = confidence < confidence_threshold
        f0_final = f0_crepe.copy()
        vuv_final = vuv_crepe.copy()
        
        f0_final[low_confidence] = f0_pyin[low_confidence]
        vuv_final[low_confidence] = vuv_pyin[low_confidence]
        
        return f0_final, vuv_final
    except:
        # CREPE ì‹¤íŒ¨ ì‹œ pYINë§Œ ì‚¬ìš©
        return f0_pyin, vuv_pyin

def extract_f0(audio, sample_rate=44100, hop_length=512, f0_min=80, f0_max=800, method='crepe_small'):
    """
    í†µí•© F0 ì¶”ì¶œ í•¨ìˆ˜
    - CREPE: ë†’ì€ ì •í™•ë„, GPU ê°€ì†
    - pYIN: ë¹ ë¥¸ ì²˜ë¦¬, CPU ìµœì í™”
    - hybrid: ìµœê³  í’ˆì§ˆ, CREPE + pYIN ê²°í•©
    """
    if isinstance(audio, torch.Tensor):
        audio = audio.cpu().numpy()
    
    # ì…ë ¥ ê²€ì¦
    if len(audio) == 0:
        return np.array([]), np.array([])
    
    try:
        if method == 'crepe_small':
            return extract_f0_crepe(audio, sample_rate, hop_length, 'small')
        elif method == 'crepe_tiny':
            return extract_f0_crepe(audio, sample_rate, hop_length, 'tiny')
        elif method == 'crepe_full':
            return extract_f0_crepe(audio, sample_rate, hop_length, 'full')
        elif method == 'hybrid':
            return extract_f0_hybrid(audio, sample_rate, hop_length)
        elif method == 'pyin':
            return extract_f0_fast_pyin(audio, sample_rate, hop_length, f0_min, f0_max)
        else:
            # ê¸°ë³¸ê°’: CREPEê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ pYIN
            if CREPE_AVAILABLE:
                return extract_f0_crepe(audio, sample_rate, hop_length, 'small')
            else:
                return extract_f0_fast_pyin(audio, sample_rate, hop_length, f0_min, f0_max)
        
        # ê¸¸ì´ ê²€ì¦
        expected_frames = 1 + len(audio) // hop_length
        if len(f0) != expected_frames:
            # ê¸¸ì´ ì¡°ì •
            if len(f0) > expected_frames:
                f0 = f0[:expected_frames]
                vuv = vuv[:expected_frames]
            else:
                # íŒ¨ë”©
                pad_length = expected_frames - len(f0)
                f0 = np.pad(f0, (0, pad_length), mode='constant', constant_values=0)
                vuv = np.pad(vuv, (0, pad_length), mode='constant', constant_values=0)
        
        
        # ê¸¸ì´ ê²€ì¦ ë° ì¡°ì •
        expected_frames = 1 + len(audio) // hop_length
        if len(f0) != expected_frames:
            if len(f0) > expected_frames:
                f0 = f0[:expected_frames]
                vuv = vuv[:expected_frames]
            else:
                pad_length = expected_frames - len(f0)
                f0 = np.pad(f0, (0, pad_length), mode='constant', constant_values=0)
                vuv = np.pad(vuv, (0, pad_length), mode='constant', constant_values=0)
        
        return f0.astype(np.float32), vuv.astype(np.float32)
        
    except Exception as e:
        print(f"F0 extraction failed with {method}: {e}")
        # í´ë°±: pYIN ì‹œë„
        if method != 'pyin':
            try:
                return extract_f0_fast_pyin(audio, sample_rate, hop_length, f0_min, f0_max)
            except:
                pass
        
        # ìµœì¢… ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        expected_frames = 1 + len(audio) // hop_length
        return np.zeros(expected_frames, dtype=np.float32), np.zeros(expected_frames, dtype=np.float32)

def compute_vuv(f0, threshold=50.0):  # ë” ë†’ì€ ì„ê³„ê°’
    """VUV ê³„ì‚° - ìµœì í™”ë¨"""
    if isinstance(f0, torch.Tensor):
        return (f0 > threshold).float()
    else:
        return (f0 > threshold).astype(np.float32)

def normalize_f0(f0, method='log', f0_min=50, f0_max=1000):  # ë” ë„“ì€ ë²”ìœ„
    """
     ìµœì í™”ëœ F0 ì •ê·œí™”
    - ë” ì•ˆì •ì ì¸ ì •ê·œí™”
    - ê·¹ê°’ ì²˜ë¦¬ ê°œì„ 
    """
    if isinstance(f0, torch.Tensor):
        is_torch = True
        device = f0.device
        f0_np = f0.cpu().numpy()
    else:
        is_torch = False
        f0_np = f0.copy()
    
    # Voiced ë§ˆìŠ¤í¬
    voiced = f0_np > 0
    
    if not np.any(voiced):
        # ëª¨ë“  í”„ë ˆì„ì´ unvoicedì¸ ê²½ìš°
        result = np.zeros_like(f0_np)
    else:
        f0_norm = np.zeros_like(f0_np)
        
        if method == 'log':
            # ë¡œê·¸ ì •ê·œí™” (ë” ì•ˆì •ì )
            f0_voiced = f0_np[voiced]
            f0_voiced = np.clip(f0_voiced, f0_min, f0_max)
            
            # ë¡œê·¸ ë³€í™˜
            log_f0 = np.log(f0_voiced + 1e-8)  # ìˆ˜ì¹˜ ì•ˆì •ì„±
            log_f0_min, log_f0_max = np.log(f0_min), np.log(f0_max)
            
            # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
            f0_norm[voiced] = 2 * (log_f0 - log_f0_min) / (log_f0_max - log_f0_min) - 1
            
        elif method == 'linear':
            # ì„ í˜• ì •ê·œí™”
            f0_voiced = f0_np[voiced]
            f0_voiced = np.clip(f0_voiced, f0_min, f0_max)
            f0_norm[voiced] = 2 * (f0_voiced - f0_min) / (f0_max - f0_min) - 1
            
        else:
            raise ValueError(f"Unknown normalization method: {method}")
        
        result = f0_norm
    
    if is_torch:
        return torch.from_numpy(result).to(device).float()
    else:
        return result.astype(np.float32)

def denormalize_f0(f0_norm, method='log', f0_min=50, f0_max=1000):
    """F0 ì—­ì •ê·œí™”"""
    if isinstance(f0_norm, torch.Tensor):
        is_torch = True
        device = f0_norm.device
        f0_np = f0_norm.cpu().numpy()
    else:
        is_torch = False
        f0_np = f0_norm.copy()
    
    if method == 'log':
        log_f0_min, log_f0_max = np.log(f0_min), np.log(f0_max)
        log_f0 = (f0_np + 1) / 2 * (log_f0_max - log_f0_min) + log_f0_min
        f0 = np.exp(log_f0) - 1e-8
    elif method == 'linear':
        f0 = (f0_np + 1) / 2 * (f0_max - f0_min) + f0_min
    else:
        raise ValueError(f"Unknown normalization method: {method}")
    
    if is_torch:
        return torch.from_numpy(f0).to(device).float()
    else:
        return f0.astype(np.float32)

class GPUAcceleratedF0Cache:
    """GPU ìµœì í™” F0 ìºì‹œ ì‹œìŠ¤í…œ"""
    
    def __init__(self, use_gpu=True, batch_size=32, model_capacity='small'):
        self.device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'
        self.batch_size = batch_size
        self.model_capacity = model_capacity
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™”
        if self.device == 'cuda':
            torch.cuda.empty_cache()
            torch.backends.cudnn.benchmark = True
        
        print(f"ğŸš€ GPU F0 Cache initialized:")
        print(f"   Device: {self.device}")
        print(f"   Batch size: {self.batch_size}")
        print(f"   Model: {self.model_capacity}")
    
    def extract_batch_f0_gpu(self, audio_batch, sample_rate=44100, hop_length=512):
        """ë°°ì¹˜ ë‹¨ìœ„ GPU F0 ì¶”ì¶œ"""
        results = []
        
        if not CREPE_AVAILABLE:
            # CREPE ì—†ìœ¼ë©´ pYIN ì‚¬ìš©
            for audio in audio_batch:
                f0, vuv = extract_f0_fast_pyin(audio, sample_rate, hop_length)
                results.append((f0, vuv))
        else:
            # CREPE ë°°ì¹˜ ì²˜ë¦¬ (GPUì—ì„œ ìë™ ê°€ì†)
            for audio in audio_batch:
                try:
                    f0, vuv = extract_f0_crepe(
                        audio, sample_rate, hop_length, self.model_capacity
                    )
                    results.append((f0, vuv))
                except Exception as e:
                    print(f"âš ï¸ CREPE failed, fallback to pYIN: {e}")
                    f0, vuv = extract_f0_fast_pyin(audio, sample_rate, hop_length)
                    results.append((f0, vuv))
        
        return results
    
    def build_cache_parallel_gpu(self, all_files, cache_dir, sample_rate=44100, hop_length=512):
        """GPU ë³‘ë ¬ F0 ìºì‹œ êµ¬ì¶•"""
        print(f"ğŸš€ Building GPU F0 cache for {len(all_files)} files...")
        
        total_processed = 0
        failed_files = []
        
        # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ë°°ì¹˜ ì²˜ë¦¬
        for i in tqdm(range(0, len(all_files), self.batch_size), desc="F0 Cache"):
            batch_files = all_files[i:i+self.batch_size]
            
            # ë°°ì¹˜ ì˜¤ë””ì˜¤ ë¡œë“œ
            audio_batch = []
            valid_files = []
            
            for file_path in batch_files:
                try:
                    # ìºì‹œ í™•ì¸
                    cache_file = cache_dir / f"{file_path.stem}_f0.npz"
                    if cache_file.exists():
                        continue
                    
                    # ì˜¤ë””ì˜¤ ë¡œë“œ
                    audio, sr = torchaudio.load(str(file_path))
                    if sr != sample_rate:
                        resampler = torchaudio.transforms.Resample(sr, sample_rate)
                        audio = resampler(audio)
                    
                    if audio.shape[0] > 1:
                        audio = audio.mean(dim=0)
                    else:
                        audio = audio.squeeze(0)
                    
                    audio_batch.append(audio.numpy())
                    valid_files.append(file_path)
                    
                except Exception as e:
                    failed_files.append((str(file_path), str(e)))
                    continue
            
            # GPU ë°°ì¹˜ F0 ì¶”ì¶œ
            if audio_batch:
                f0_results = self.extract_batch_f0_gpu(
                    audio_batch, sample_rate, hop_length
                )
                
                # ìºì‹œ ì €ì¥
                for (f0, vuv), file_path in zip(f0_results, valid_files):
                    try:
                        cache_file = cache_dir / f"{file_path.stem}_f0.npz"
                        f0_norm = normalize_f0(f0, method='log')
                        
                        np.savez_compressed(
                            cache_file,
                            f0=f0, 
                            f0_normalized=f0_norm, 
                            vuv=vuv
                        )
                        total_processed += 1
                    except Exception as e:
                        failed_files.append((str(file_path), f"Cache save failed: {e}"))
        
        # ê²°ê³¼ ë³´ê³ 
        print(f"âœ… GPU F0 cache completed:")
        print(f"   Processed: {total_processed} files")
        if failed_files:
            print(f"   Failed: {len(failed_files)} files")
            for file_path, error in failed_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                print(f"      {Path(file_path).name}: {error}")
        
        return total_processed, failed_files

class OptimizedVoiceConversionDataset(Dataset):
    """
     ìµœì í™”ëœ Voice Conversion Dataset
    - ë©€í‹°í”„ë¡œì„¸ì‹± F0 ì¶”ì¶œ
    - ìºì‹± ì‹œìŠ¤í…œ
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”©
    - ë” ë¹ ë¥¸ ë°ì´í„° ì²˜ë¦¬
    """
    
    def __init__(self, data_dir, sample_rate=44100, waveform_length=16384, 
                 channels=2, min_files_per_speaker=5, extract_f0=True, 
                 hop_length=512, f0_method='crepe_small', use_cache=True, 
                 max_workers=None, use_gpu_cache=True, gpu_batch_size=32):
        
        self.data_dir = Path(data_dir)
        self.sample_rate = sample_rate
        self.waveform_length = waveform_length
        self.channels = channels
        self.min_files_per_speaker = min_files_per_speaker
        self.extract_f0 = extract_f0
        self.hop_length = hop_length
        self.f0_method = f0_method
        self.use_cache = use_cache
        self.use_gpu_cache = use_gpu_cache
        self.gpu_batch_size = gpu_batch_size
        
        #  ìºì‹œ ë””ë ‰í† ë¦¬
        self.cache_dir = self.data_dir / '.cache'
        if self.use_cache:
            self.cache_dir.mkdir(exist_ok=True)
        
        # GPU ìºì‹œ ì´ˆê¸°í™”
        if self.use_gpu_cache and self.extract_f0:
            model_capacity = 'small' if 'crepe' in self.f0_method else 'small'
            if 'tiny' in self.f0_method:
                model_capacity = 'tiny'
            elif 'full' in self.f0_method:
                model_capacity = 'full'
            
            self.gpu_cache = GPUAcceleratedF0Cache(
                use_gpu=True,
                batch_size=self.gpu_batch_size,
                model_capacity=model_capacity
            )
        else:
            self.gpu_cache = None
        
        # ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì •
        self.max_workers = max_workers or min(8, mp.cpu_count())
        
        print(f"ğŸµ Initializing optimized dataset:")
        print(f"   Cache: {'âœ“ Enabled' if self.use_cache else 'âœ— Disabled'}")
        print(f"   F0 extraction: {'âœ“ Enabled' if self.extract_f0 else 'âœ— Disabled'}")
        print(f"   F0 method: {self.f0_method}")
        print(f"   GPU cache: {'âœ“ Enabled' if self.use_gpu_cache else 'âœ— Disabled'}")
        print(f"   Max workers: {self.max_workers}")
        
        # ë°ì´í„° ìŠ¤ìº”
        self._scan_dataset()
        
        # F0 ìºì‹œ ì¤€ë¹„
        if self.extract_f0:
            self._prepare_f0_cache()
    
    def _scan_dataset(self):
        """ ìµœì í™”ëœ ë°ì´í„°ì…‹ ìŠ¤ìº”"""
        start_time = time.time()
        
        self.speakers = []
        self.speaker_files = {}
        
        # ë³‘ë ¬ë¡œ í™”ì í´ë” ìŠ¤ìº”
        speaker_dirs = [d for d in self.data_dir.iterdir() if d.is_dir() and not d.name.startswith('.')]
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            results = list(executor.map(self._scan_speaker_dir, speaker_dirs))
        
        # ê²°ê³¼ ì§‘ê³„
        for speaker_name, audio_files in results:
            if speaker_name and len(audio_files) >= self.min_files_per_speaker:
                self.speakers.append(speaker_name)
                self.speaker_files[speaker_name] = sorted(audio_files)
                print(f" {speaker_name}: {len(audio_files)} files")
            elif speaker_name:
                print(f" {speaker_name}: {len(audio_files)} files (< {self.min_files_per_speaker}, skipped)")
        
        # í™”ì ID ë§¤í•‘
        self.speaker_to_id = {spk: i for i, spk in enumerate(sorted(self.speakers))}
        self.id_to_speaker = {i: spk for spk, i in self.speaker_to_id.items()}
        
        # í›ˆë ¨ í˜ì–´ ìƒì„±
        self._generate_training_pairs()
        
        scan_time = time.time() - start_time
        print(f" Dataset scan completed in {scan_time:.2f}s")
    
    def _scan_speaker_dir(self, speaker_dir):
        """ê°œë³„ í™”ì ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
        speaker_name = speaker_dir.name
        audio_files = []
        
        for ext in ['*.wav', '*.mp3', '*.flac', '*.m4a']:
            audio_files.extend(list(speaker_dir.glob(ext)))
        
        return speaker_name, audio_files
    
    def _generate_training_pairs(self):
        """í›ˆë ¨ í˜ì–´ ìƒì„± - ìµœì í™”ë¨"""
        self.training_pairs = []
        
        for source_speaker in self.speakers:
            source_files = self.speaker_files[source_speaker]
            
            for target_speaker in self.speakers:
                if target_speaker != source_speaker:
                    target_speaker_id = self.speaker_to_id[target_speaker]
                    
                    for source_file in source_files:
                        self.training_pairs.append({
                            'source_file': source_file,
                            'source_speaker': source_speaker,
                            'target_speaker': target_speaker,
                            'target_speaker_id': target_speaker_id
                        })
        
        print(f" Generated {len(self.training_pairs)} training pairs")
    
    def _prepare_f0_cache(self):
        """F0 ìºì‹œ ì¤€ë¹„"""
        if not self.use_cache:
            return
        
        cache_info_file = self.cache_dir / 'f0_cache_info.json'
        
        if cache_info_file.exists():
            with open(cache_info_file, 'r') as f:
                cache_info = json.load(f)
            
            # ìºì‹œ ìœ íš¨ì„± ê²€ì¦
            if (cache_info.get('sample_rate') == self.sample_rate and
                cache_info.get('hop_length') == self.hop_length and
                cache_info.get('f0_method') == self.f0_method):
                print(" F0 cache is valid")
                return
        
        print(" Building F0 cache...")
        self._build_f0_cache()
    
    def _build_f0_cache(self):
        """F0 ìºì‹œ êµ¬ì¶• - GPU ê°€ì† ì§€ì›"""
        all_files = []
        for files in self.speaker_files.values():
            all_files.extend(files)
        
        print(f"ğŸµ Processing {len(all_files)} files for F0 cache...")
        
        if self.use_gpu_cache and self.gpu_cache:
            # GPU ê°€ì† ìºì‹œ êµ¬ì¶•
            total_processed, failed_files = self.gpu_cache.build_cache_parallel_gpu(
                all_files, self.cache_dir, self.sample_rate, self.hop_length
            )
        else:
            # ê¸°ì¡´ CPU ë©€í‹°ìŠ¤ë ˆë”© ë°©ì‹
            print("ğŸ’» Using CPU multiprocessing for F0 cache...")
            batch_size = 50
            for i in tqdm(range(0, len(all_files), batch_size), desc="F0 Cache"):
                batch_files = all_files[i:i+batch_size]
                
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    executor.map(self._extract_and_cache_f0, batch_files)
        
        # ìºì‹œ ì •ë³´ ì €ì¥
        cache_info = {
            'sample_rate': self.sample_rate,
            'hop_length': self.hop_length,
            'f0_method': self.f0_method,
            'total_files': len(all_files),
            'gpu_accelerated': self.use_gpu_cache and self.gpu_cache is not None
        }
        
        with open(self.cache_dir / 'f0_cache_info.json', 'w') as f:
            json.dump(cache_info, f)
        
        print("âœ… F0 cache built successfully")
    
    def _extract_and_cache_f0(self, audio_file):
        """ê°œë³„ íŒŒì¼ì˜ F0 ì¶”ì¶œ ë° ìºì‹œ"""
        cache_file = self.cache_dir / f"{audio_file.stem}_f0.npz"
        
        if cache_file.exists():
            return  # ì´ë¯¸ ìºì‹œë¨
        
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ (ì§§ê²Œ)
            waveform, sr = torchaudio.load(audio_file)
            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
                waveform = resampler(waveform)
            
            # ëª¨ë…¸ ë³€í™˜
            if waveform.size(0) > 1:
                waveform = waveform.mean(dim=0)
            else:
                waveform = waveform.squeeze(0)
            
            # F0 ì¶”ì¶œ - ë©”ì†Œë“œì— ë”°ë¼ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©
            if self.f0_method.startswith('crepe'):
                f0, vuv = extract_f0(
                    waveform.numpy(),
                    sample_rate=self.sample_rate,
                    hop_length=self.hop_length,
                    method=self.f0_method
                )
            else:
                f0, vuv = extract_f0(
                    waveform.numpy(),
                    sample_rate=self.sample_rate,
                    hop_length=self.hop_length,
                    f0_min=80, f0_max=800,
                    method=self.f0_method
                )
            
            # ì •ê·œí™”
            f0_normalized = normalize_f0(f0, method='log')
            
            # ìºì‹œ ì €ì¥
            np.savez_compressed(
                cache_file,
                f0=f0,
                f0_normalized=f0_normalized,
                vuv=vuv
            )
            
        except Exception as e:
            print(f" Failed to cache F0 for {audio_file}: {e}")
    
    def _load_cached_f0(self, audio_file):
        """ìºì‹œëœ F0 ë¡œë“œ"""
        if not self.use_cache:
            return None
        
        cache_file = self.cache_dir / f"{audio_file.stem}_f0.npz"
        
        if cache_file.exists():
            try:
                data = np.load(cache_file)
                return {
                    'f0': data['f0'],
                    'f0_normalized': data['f0_normalized'],
                    'vuv': data['vuv']
                }
            except Exception as e:
                print(f" Failed to load cached F0 for {audio_file}: {e}")
        
        return None
    
    def __len__(self):
        return len(self.training_pairs)
    
    def __getitem__(self, idx):
        pair = self.training_pairs[idx]
        
        #  ì˜¤ë””ì˜¤ ë¡œë”©
        source_waveform = self._load_audio(pair['source_file'])
        
        # íƒ€ê²Ÿ ì˜¤ë””ì˜¤ (ëœë¤ ì„ íƒ)
        target_files = self.speaker_files[pair['target_speaker']]
        target_file = random.choice(target_files)
        target_waveform = self._load_audio(target_file)
        
        result = {
            'source_waveform': source_waveform,
            'target_waveform': target_waveform,
            'target_speaker_id': torch.tensor(pair['target_speaker_id'], dtype=torch.long),
            'source_speaker': pair['source_speaker'],
            'target_speaker': pair['target_speaker'],
            'source_file': str(pair['source_file']),
            'target_file': str(target_file)
        }
        
        #  F0 ì²˜ë¦¬
        if self.extract_f0:
            # ìºì‹œì—ì„œ ì‹œë„
            f0_data = self._load_cached_f0(target_file)
            
            if f0_data is not None:
                # ìºì‹œëœ ë°ì´í„° ì‚¬ìš©
                f0_normalized = f0_data['f0_normalized']
                vuv = f0_data['vuv']
            else:
                # ì‹¤ì‹œê°„ ì¶”ì¶œ
                if target_waveform.dim() == 2:
                    target_mono = target_waveform.mean(dim=0)
                else:
                    target_mono = target_waveform
                
                try:
                    # F0 ì¶”ì¶œ - ë©”ì†Œë“œì— ë”°ë¼ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©
                    if self.f0_method.startswith('crepe'):
                        f0, vuv = extract_f0(
                            target_mono.numpy(),
                            sample_rate=self.sample_rate,
                            hop_length=self.hop_length,
                            method=self.f0_method
                        )
                    else:
                        f0, vuv = extract_f0(
                            target_mono.numpy(),
                            sample_rate=self.sample_rate,
                            hop_length=self.hop_length,
                            f0_min=80, f0_max=800,
                            method=self.f0_method
                        )
                    f0_normalized = normalize_f0(f0, method='log')
                except Exception as e:
                    print(f" F0 extraction failed: {e}")
                    # ê¸°ë³¸ê°’ ì‚¬ìš©
                    default_frames = self.waveform_length // self.hop_length + 1
                    f0_normalized = np.zeros(default_frames, dtype=np.float32)
                    vuv = np.zeros(default_frames, dtype=np.float32)
            
            result['f0_target'] = torch.from_numpy(f0_normalized).float()
            result['vuv_target'] = torch.from_numpy(vuv).float()
        
        return result
    
    def _load_audio(self, file_path):
        """ ìµœì í™”ëœ ì˜¤ë””ì˜¤ ë¡œë”©"""
        try:
            #  torchaudioë¡œ ë¹ ë¥¸ ë¡œë”©
            waveform, sr = torchaudio.load(str(file_path))
            
            # ë¦¬ìƒ˜í”Œë§ (í•„ìš”ì‹œ)
            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
                waveform = resampler(waveform)
            
            # ì±„ë„ ì²˜ë¦¬
            if self.channels == 2:
                if waveform.size(0) == 1:
                    waveform = waveform.repeat(2, 1)
                elif waveform.size(0) > 2:
                    waveform = waveform[:2]
            else:
                if waveform.size(0) > 1:
                    waveform = waveform.mean(dim=0, keepdim=True)
            
            # ê¸¸ì´ ì¡°ì •
            if self.channels == 2:
                waveform = self._fix_length(waveform, self.waveform_length, dim=1)
            else:
                waveform = self._fix_length(waveform.squeeze(0), self.waveform_length, dim=0)
            
            return waveform
            
        except Exception as e:
            print(f" Failed to load audio {file_path}: {e}")
            # ê¸°ë³¸ ë…¸ì´ì¦ˆ ë°˜í™˜
            if self.channels == 2:
                return torch.randn(2, self.waveform_length) * 0.01
            else:
                return torch.randn(self.waveform_length) * 0.01
    
    def _fix_length(self, waveform, target_length, dim=0):
        """ê¸¸ì´ ì¡°ì • - ìµœì í™”ë¨"""
        current_length = waveform.size(dim)
        
        if current_length > target_length:
            # ëœë¤ í¬ë¡­ (ë” íš¨ìœ¨ì )
            start = torch.randint(0, current_length - target_length + 1, (1,)).item()
            if dim == 0:
                return waveform[start:start + target_length]
            else:
                return waveform[:, start:start + target_length]
        elif current_length < target_length:
            # ì œë¡œ íŒ¨ë”©
            pad_length = target_length - current_length
            if dim == 0:
                return torch.cat([waveform, torch.zeros(pad_length)], dim=0)
            else:
                return torch.cat([waveform, torch.zeros(waveform.size(0), pad_length)], dim=1)
        else:
            return waveform
    
    def get_speaker_info(self):
        """í™”ì ì •ë³´ ë°˜í™˜"""
        return {
            'speakers': self.speakers,
            'speaker_to_id': self.speaker_to_id,
            'id_to_speaker': self.id_to_speaker,
            'total_speakers': len(self.speakers),
            'total_pairs': len(self.training_pairs),
            'files_per_speaker': {spk: len(files) for spk, files in self.speaker_files.items()}
        }
    
    def print_sample_pairs(self, num_samples=5):
        """ìƒ˜í”Œ í˜ì–´ ì¶œë ¥"""
        print(f"\n Sample training pairs:")
        for i in range(min(num_samples, len(self.training_pairs))):
            pair = self.training_pairs[i]
            print(f"   {i+1}. {pair['source_speaker']} â†’ {pair['target_speaker']}")
            print(f"       {Path(pair['source_file']).name}")
        
        if self.extract_f0:
            print(f"    F0 conditioning enabled with cache")
        print()

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
VoiceConversionDataset = OptimizedVoiceConversionDataset

def optimized_collate_fn(batch):
    """ ìµœì í™”ëœ collate í•¨ìˆ˜"""
    # ì²« ë²ˆì§¸ ì•„ì´í…œì—ì„œ ì°¨ì› í™•ì¸
    first_source = batch[0]['source_waveform']
    is_stereo = first_source.dim() == 2
    batch_size = len(batch)
    
    # íš¨ìœ¨ì ì¸ ìŠ¤íƒí‚¹
    if is_stereo:
        source_waveforms = torch.stack([item['source_waveform'] for item in batch])
        target_waveforms = torch.stack([item['target_waveform'] for item in batch])
    else:
        source_waveforms = torch.stack([item['source_waveform'] for item in batch])
        target_waveforms = torch.stack([item['target_waveform'] for item in batch])
    
    target_speaker_ids = torch.stack([item['target_speaker_id'] for item in batch])
    
    result = {
        'source_waveform': source_waveforms,
        'target_waveform': target_waveforms,
        'target_speaker_id': target_speaker_ids,
        'source_speakers': [item['source_speaker'] for item in batch],
        'target_speakers': [item['target_speaker'] for item in batch]
    }
    
    #  F0/VUV ì²˜ë¦¬ (ìµœì í™”ë¨)
    if 'f0_target' in batch[0]:
        # ë°°ì¹˜ ë‚´ ìµœëŒ€ ê¸¸ì´ ì°¾ê¸°
        max_f0_len = max(item['f0_target'].size(0) for item in batch)
        
        # ë¯¸ë¦¬ í• ë‹¹
        f0_targets = torch.zeros(batch_size, max_f0_len)
        vuv_targets = torch.zeros(batch_size, max_f0_len)
        
        for i, item in enumerate(batch):
            f0 = item['f0_target']
            vuv = item['vuv_target']
            
            # ê¸¸ì´ ì¡°ì •
            actual_len = min(f0.size(0), max_f0_len)
            f0_targets[i, :actual_len] = f0[:actual_len]
            vuv_targets[i, :actual_len] = vuv[:actual_len]
        
        result['f0_target'] = f0_targets
        result['vuv_target'] = vuv_targets
    
    return result

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
collate_fn = optimized_collate_fn

#  ì¶”ê°€ ìµœì í™” ìœ í‹¸ë¦¬í‹°ë“¤

def create_optimized_train_val_split(dataset_root, train_ratio=0.8, use_symlinks=True):
    """
    ìµœì í™”ëœ train/val ë¶„í• 
    - ì‹¬ë³¼ë¦­ ë§í¬ ì‚¬ìš©ìœ¼ë¡œ ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½
    - ë³‘ë ¬ ì²˜ë¦¬
    """
    dataset_root = Path(dataset_root)
    train_dir = dataset_root / 'train'
    val_dir = dataset_root / 'val'
    
    # ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì •ë¦¬
    if train_dir.exists():
        shutil.rmtree(train_dir)
    if val_dir.exists():
        shutil.rmtree(val_dir)
    
    train_dir.mkdir()
    val_dir.mkdir()
    
    def process_speaker(speaker_dir):
        if not speaker_dir.is_dir() or speaker_dir.name in ['train', 'val', '.cache']:
            return None, 0, 0
        
        speaker_name = speaker_dir.name
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ìˆ˜ì§‘
        audio_files = []
        for ext in ['*.wav', '*.mp3', '*.flac', '*.m4a']:
            audio_files.extend(list(speaker_dir.glob(ext)))
        
        if len(audio_files) < 2:
            return speaker_name, 0, 0
        
        # ë¶„í• 
        random.shuffle(audio_files)
        split_idx = int(len(audio_files) * train_ratio)
        
        train_files = audio_files[:split_idx]
        val_files = audio_files[split_idx:]
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        (train_dir / speaker_name).mkdir()
        (val_dir / speaker_name).mkdir()
        
        # íŒŒì¼ ë³µì‚¬ ë˜ëŠ” ë§í¬
        for file in train_files:
            dest = train_dir / speaker_name / file.name
            if use_symlinks:
                dest.symlink_to(file.absolute())
            else:
                shutil.copy2(file, dest)
        
        for file in val_files:
            dest = val_dir / speaker_name / file.name
            if use_symlinks:
                dest.symlink_to(file.absolute())
            else:
                shutil.copy2(file, dest)
        
        return speaker_name, len(train_files), len(val_files)
    
    # ë³‘ë ¬ ì²˜ë¦¬
    speaker_dirs = [d for d in dataset_root.iterdir() if d.is_dir()]
    
    with ThreadPoolExecutor(max_workers=min(8, mp.cpu_count())) as executor:
        results = list(executor.map(process_speaker, speaker_dirs))
    
    # ê²°ê³¼ ì¶œë ¥
    total_train = 0
    total_val = 0
    
    for speaker_name, train_count, val_count in results:
        if speaker_name and (train_count > 0 or val_count > 0):
            print(f" {speaker_name}: {train_count} train, {val_count} val")
            total_train += train_count
            total_val += val_count
    
    print(f" Optimized Train/Val split completed!")
    print(f"    Total train files: {total_train}")
    print(f"    Total val files: {total_val}")
    print(f"    Using {'symlinks' if use_symlinks else 'copies'}")

def benchmark_dataset_loading(dataset, num_samples=100):
    """ë°ì´í„°ì…‹ ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    print(f" Benchmarking dataset loading ({num_samples} samples)...")
    
    start_time = time.time()
    
    for i in range(min(num_samples, len(dataset))):
        sample = dataset[i]
    
    end_time = time.time()
    total_time = end_time - start_time
    avg_time = total_time / num_samples
    
    print(f" Loading benchmark results:")
    print(f"   Total time: {total_time:.2f}s")
    print(f"   Average per sample: {avg_time*1000:.2f}ms")
    print(f"   Samples per second: {num_samples/total_time:.1f}")
    
    return avg_time

# ìµœì í™”ëœ F0 ì¶”ì¶œ ì„¤ì •
F0_CONFIG = {
    # ê¸°ë³¸ ì„¤ì •
    'extract_f0': True,
    'f0_method': 'crepe_small',  # ì¶”ì²œ: ì†ë„-ì •í™•ë„ ê· í˜•
    'use_gpu_f0': True,
    'f0_batch_size': 32,
    
    # ë©”ì†Œë“œë³„ ì„¤ì •
    'methods': {
        'crepe_tiny': {
            'description': 'ê°€ì¥ ë¹ ë¦„ (3ë°°), ì•½ê°„ ì •í™•ë„ í•˜ë½',
            'speed': 'fastest',
            'accuracy': 'good'
        },
        'crepe_small': {
            'description': 'ê· í˜•ì¡íŒ ì„ íƒ (2ë°° ë¹ ë¦„, ë†’ì€ ì •í™•ë„)',
            'speed': 'fast',
            'accuracy': 'high'
        },
        'crepe_full': {
            'description': 'ìµœê³  ì •í™•ë„, ê°€ì¥ ëŠë¦¼',
            'speed': 'slow',
            'accuracy': 'highest'
        },
        'hybrid': {
            'description': 'CREPE + pYIN ê²°í•©, ìµœê³  í’ˆì§ˆ',
            'speed': 'medium',
            'accuracy': 'highest'
        },
        'pyin': {
            'description': 'CPU ìµœì í™”, CREPE ì—†ì„ ë•Œ ì‚¬ìš©',
            'speed': 'medium',
            'accuracy': 'medium'
        }
    }
}

def get_f0_config():
    """F0 ì„¤ì • ì •ë³´ ë°˜í™˜"""
    return F0_CONFIG

def print_f0_methods():
    """ì‚¬ìš© ê°€ëŠ¥í•œ F0 ì¶”ì¶œ ë°©ë²• ì¶œë ¥"""
    print("ğŸµ Available F0 extraction methods:")
    print(f"   CREPE available: {'âœ“' if CREPE_AVAILABLE else 'âœ—'}")
    print(f"   SciPy available: {'âœ“' if SCIPY_AVAILABLE else 'âœ—'}")
    print()
    
    for method, info in F0_CONFIG['methods'].items():
        status = "âœ“" if method == 'pyin' or CREPE_AVAILABLE else "âœ— (needs CREPE)"
        print(f"   {status} {method:12} - {info['description']}")
        print(f"      Speed: {info['speed']:8} | Accuracy: {info['accuracy']}")
    print()
    print("Recommended:")
    print("  â€¢ For fastest: crepe_tiny")
    print("  â€¢ For balanced: crepe_small (default)")
    print("  â€¢ For best quality: hybrid")
    print("  â€¢ For CPU only: pyin")

# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
def create_optimized_dataset_with_f0(data_dir, **kwargs):
    """
    ìµœì í™”ëœ F0 ìºì‹œê°€ í¬í•¨ëœ ë°ì´í„°ì…‹ ìƒì„±
    
    Usage:
        # GPU ê°€ì† CREPE (ì¶”ì²œ)
        dataset = create_optimized_dataset_with_f0(
            'path/to/data',
            f0_method='crepe_small',
            use_gpu_cache=True,
            gpu_batch_size=32
        )
        
        # ìµœê³  í’ˆì§ˆ í•˜ì´ë¸Œë¦¬ë“œ
        dataset = create_optimized_dataset_with_f0(
            'path/to/data',
            f0_method='hybrid',
            use_gpu_cache=True
        )
        
        # CPU ì „ìš©
        dataset = create_optimized_dataset_with_f0(
            'path/to/data',
            f0_method='pyin',
            use_gpu_cache=False
        )
    """
    # ê¸°ë³¸ê°’ ì„¤ì •
    config = F0_CONFIG.copy()
    config.update(kwargs)
    
    return OptimizedVoiceConversionDataset(data_dir, **config)