#!/usr/bin/env python3
"""
ğŸš€ Ultra-Optimized Voice Conversion Inference
- AMP FP16 í˜¼í•© ì •ë°€ë„
- Rectified Flow ë¹ ë¥¸ ìƒ˜í”Œë§
- F0 ì¡°ê±´ë¶€ ìƒì„±
- ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
- ì»´íŒŒì¼ ìµœì í™”
"""

import torch
import torchaudio
import argparse
from pathlib import Path
import time
import numpy as np
from tqdm import tqdm
import warnings
from typing import Optional, Tuple, List

from model import VoiceConversionModel
from utils import extract_f0, normalize_f0

warnings.filterwarnings("ignore", category=UserWarning)

class OptimizedInferenceEngine:
    """
    ğŸš€ ìµœì í™”ëœ ì¶”ë¡  ì—”ì§„
    - AMP FP16 ìµœì í™”
    - ë°°ì¹˜ ì²˜ë¦¬
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
    - ë™ì  ì²­í¬ í¬ê¸°
    """
    
    def __init__(self, model_path: str, device: str = 'auto', use_amp: bool = True,
                 compile_model: bool = True):
        
        self.device = self._setup_device(device)
        self.use_amp = use_amp and torch.cuda.is_available()
        
        print(f"ğŸš€ Initializing OptimizedInferenceEngine:")
        print(f"   Device: {self.device}")
        print(f"   AMP FP16: {'âœ… Enabled' if self.use_amp else 'âŒ Disabled'}")
        
        # ëª¨ë¸ ë¡œë“œ
        self.model, self.config = self._load_model(model_path)
        
        # ì»´íŒŒì¼ ìµœì í™”
        if compile_model:
            self._compile_model()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.inference_times = []
        self.memory_usage = []
        
        print("âœ… Inference engine ready!")
    
    def _setup_device(self, device: str) -> torch.device:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device == 'auto':
            if torch.cuda.is_available():
                device = 'cuda'
            else:
                device = 'cpu'
        
        device = torch.device(device)
        
        if device.type == 'cuda':
            # CUDA ìµœì í™”
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            print(f"ğŸ”¥ CUDA optimizations enabled")
        
        return device
    
    def _load_model(self, model_path: str) -> Tuple[VoiceConversionModel, dict]:
        """ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ“¦ Loading model from {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        config = checkpoint.get('config', {})
        
        # ëª¨ë¸ ìƒì„±
        model = VoiceConversionModel(
            d_model=config.get('d_model', 768),
            ssm_layers=config.get('ssm_layers', 3),
            flow_steps=config.get('flow_steps', 20),
            n_speakers=config.get('n_speakers', 256),
            waveform_length=config.get('waveform_length', 16384),
            use_retrieval=config.get('use_retrieval', True),
            lora_rank=config.get('lora_rank', 16),
            adapter_dim=config.get('adapter_dim', 64),
            use_f0_conditioning=config.get('use_f0_conditioning', True)
        ).to(self.device)
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        # ğŸ”¥ Half precisionìœ¼ë¡œ ë³€í™˜ (AMP ì‚¬ìš©ì‹œ)
        if self.use_amp:
            model = model.half()
            print("ğŸ”¥ Model converted to FP16")
        
        print("âœ… Model loaded successfully")
        return model, config
    
    def _compile_model(self):
        """ëª¨ë¸ ì»´íŒŒì¼"""
        try:
            print("ğŸš€ Compiling model for optimization...")
            self.model.compile_model()
            print("âœ… Model compilation completed")
        except Exception as e:
            print(f"âš ï¸ Model compilation failed: {e}")
    
    @torch.no_grad()
    @torch.cuda.amp.autocast()
    def convert_single_chunk(self, 
                           source_chunk: torch.Tensor,
                           target_speaker_id: int,
                           f0_chunk: Optional[torch.Tensor] = None,
                           vuv_chunk: Optional[torch.Tensor] = None,
                           method: str = 'fast_rectified',
                           num_steps: int = 6) -> torch.Tensor:
        """
        ğŸš€ ë‹¨ì¼ ì²­í¬ ë³€í™˜ (ìµœì í™”ë¨)
        """
        # ì…ë ¥ ì¤€ë¹„
        if source_chunk.dim() == 2:
            source_chunk = source_chunk.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        
        target_speaker_tensor = torch.tensor([target_speaker_id], device=self.device)
        
        # F0 ì¡°ê±´ ì¤€ë¹„
        f0_target = None
        vuv_target = None
        
        if f0_chunk is not None and vuv_chunk is not None:
            f0_target = f0_chunk.unsqueeze(0) if f0_chunk.dim() == 1 else f0_chunk
            vuv_target = vuv_chunk.unsqueeze(0) if vuv_chunk.dim() == 1 else vuv_chunk
        
        # ì¶”ë¡ 
        with torch.cuda.amp.autocast(enabled=self.use_amp):
            result = self.model(
                source_waveform=source_chunk,
                target_speaker_id=target_speaker_tensor,
                f0_target=f0_target,
                vuv_target=vuv_target,
                training=False,
                inference_method=method,
                num_steps=num_steps
            )
        
        return result['converted_waveform'].squeeze(0)  # ë°°ì¹˜ ì°¨ì› ì œê±°
    
    def load_and_preprocess_audio(self, audio_path: str, 
                                target_sample_rate: int = 44100) -> torch.Tensor:
        """ğŸ”¥ ìµœì í™”ëœ ì˜¤ë””ì˜¤ ë¡œë”© ë° ì „ì²˜ë¦¬"""
        print(f"ğŸµ Loading audio: {audio_path}")
        
        # ë¹ ë¥¸ ë¡œë”©
        waveform, sr = torchaudio.load(audio_path)
        
        # ë¦¬ìƒ˜í”Œë§
        if sr != target_sample_rate:
            resampler = torchaudio.transforms.Resample(sr, target_sample_rate)
            waveform = resampler(waveform)
            print(f"ğŸ”„ Resampled: {sr}Hz â†’ {target_sample_rate}Hz")
        
        # ìŠ¤í…Œë ˆì˜¤ ë³€í™˜
        if waveform.size(0) == 1:
            waveform = waveform.repeat(2, 1)
        elif waveform.size(0) > 2:
            waveform = waveform[:2]
        
        # ë””ë°”ì´ìŠ¤ ì´ë™
        waveform = waveform.to(self.device)
        
        # FP16 ë³€í™˜
        if self.use_amp:
            waveform = waveform.half()
        
        duration = waveform.size(1) / target_sample_rate
        print(f"âœ… Audio loaded: {waveform.shape}, Duration: {duration:.2f}s")
        
        return waveform
    
    def extract_f0_features(self, waveform: torch.Tensor, 
                          sample_rate: int = 44100) -> Tuple[torch.Tensor, torch.Tensor]:
        """ğŸµ F0 íŠ¹ì„± ì¶”ì¶œ"""
        # ëª¨ë…¸ ë³€í™˜
        if waveform.dim() == 2:
            mono_waveform = waveform.mean(dim=0)
        else:
            mono_waveform = waveform
        
        # CPUì—ì„œ F0 ì¶”ì¶œ
        audio_np = mono_waveform.cpu().float().numpy()
        
        # F0 ì¶”ì¶œ
        f0, vuv = extract_f0(
            audio_np,
            sample_rate=sample_rate,
            hop_length=512,
            method='pyin'
        )
        
        # ì •ê·œí™”
        f0_normalized = normalize_f0(f0, method='log')
        
        # í…ì„œ ë³€í™˜
        f0_tensor = torch.from_numpy(f0_normalized).to(self.device)
        vuv_tensor = torch.from_numpy(vuv).to(self.device)
        
        if self.use_amp:
            f0_tensor = f0_tensor.half()
            vuv_tensor = vuv_tensor.half()
        
        return f0_tensor, vuv_tensor
    
    def split_audio_with_f0(self, waveform: torch.Tensor, 
                          f0: torch.Tensor, vuv: torch.Tensor,
                          chunk_length: int = 16384, 
                          overlap: int = 2048) -> List[dict]:
        """ğŸ”¥ F0ì™€ í•¨ê»˜ ì˜¤ë””ì˜¤ ë¶„í• """
        channels, total_length = waveform.shape
        step_size = chunk_length - overlap
        
        chunks = []
        
        start = 0
        while start < total_length:
            end = min(start + chunk_length, total_length)
            
            # ì˜¤ë””ì˜¤ ì²­í¬
            audio_chunk = waveform[:, start:end]
            
            # íŒ¨ë”©
            if audio_chunk.size(1) < chunk_length:
                pad_length = chunk_length - audio_chunk.size(1)
                audio_chunk = torch.cat([
                    audio_chunk, 
                    torch.zeros(channels, pad_length, device=waveform.device, dtype=waveform.dtype)
                ], dim=1)
            
            # F0 ì²­í¬ (ì‹œê°„ ëŒ€ì‘)
            hop_length = 512
            f0_start = start // hop_length
            f0_end = min(f0_start + chunk_length // hop_length + 1, f0.size(0))
            
            f0_chunk = f0[f0_start:f0_end]
            vuv_chunk = vuv[f0_start:f0_end]
            
            chunks.append({
                'audio': audio_chunk,
                'f0': f0_chunk,
                'vuv': vuv_chunk,
                'start': start,
                'end': end,
                'actual_length': min(chunk_length, total_length - start)
            })
            
            start += step_size
            
            if end >= total_length:
                break
        
        print(f"ğŸ“Š Split into {len(chunks)} chunks (overlap: {overlap})")
        return chunks
    
    def merge_chunks_with_crossfade(self, chunks: List[torch.Tensor], 
                                   positions: List[dict],
                                   overlap: int = 2048) -> torch.Tensor:
        """ğŸ”¥ í¬ë¡œìŠ¤í˜ì´ë“œë¡œ ì²­í¬ ë³‘í•©"""
        if not chunks:
            return torch.empty(0)
        
        # ì´ ê¸¸ì´ ê³„ì‚°
        total_length = max(pos['end'] for pos in positions)
        
        # ì¶œë ¥ ì´ˆê¸°í™”
        merged_audio = torch.zeros(total_length, device=chunks[0].device, dtype=chunks[0].dtype)
        weights = torch.zeros(total_length, device=chunks[0].device, dtype=chunks[0].dtype)
        
        fade_length = min(overlap // 2, 512)
        
        for i, (chunk, pos) in enumerate(zip(chunks, positions)):
            start, end = pos['start'], pos['end']
            actual_length = pos['actual_length']
            
            # ì‹¤ì œ ê¸¸ì´ë§Œí¼ ì‚¬ìš©
            if actual_length < chunk.size(0):
                chunk = chunk[:actual_length]
            
            actual_end = min(start + chunk.size(0), total_length)
            chunk_trimmed = chunk[:actual_end - start]
            
            # í˜ì´ë“œ ì¸/ì•„ì›ƒ
            if i > 0 and fade_length > 0:
                fade_in = torch.linspace(0, 1, fade_length, device=chunk.device, dtype=chunk.dtype)
                chunk_trimmed[:fade_length] *= fade_in
            
            if i < len(chunks) - 1 and fade_length > 0:
                fade_out = torch.linspace(1, 0, fade_length, device=chunk.device, dtype=chunk.dtype)
                chunk_trimmed[-fade_length:] *= fade_out
            
            # ë³‘í•©
            merged_audio[start:actual_end] += chunk_trimmed
            weights[start:actual_end] += 1.0
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        weights[weights == 0] = 1.0
        merged_audio = merged_audio / weights
        
        return merged_audio
    
    def convert_full_audio(self, 
                         audio_path: str,
                         target_speaker_id: int,
                         output_path: str,
                         chunk_length: int = 16384,
                         overlap: int = 2048,
                         method: str = 'fast_rectified',
                         num_steps: int = 6,
                         use_f0: bool = True) -> dict:
        """
        ğŸš€ ì „ì²´ ì˜¤ë””ì˜¤ ë³€í™˜ (ìµœì í™”ë¨)
        """
        start_time = time.time()
        
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        waveform = self.load_and_preprocess_audio(audio_path)
        
        # F0 ì¶”ì¶œ
        f0, vuv = None, None
        if use_f0 and self.config.get('use_f0_conditioning', True):
            print("ğŸµ Extracting F0 features...")
            f0, vuv = self.extract_f0_features(waveform)
            print(f"âœ… F0 extracted: {f0.shape}")
        
        # ì²­í¬ ë¶„í• 
        if use_f0 and f0 is not None:
            chunks = self.split_audio_with_f0(waveform, f0, vuv, chunk_length, overlap)
        else:
            chunks = self._split_audio_simple(waveform, chunk_length, overlap)
        
        # ë³€í™˜ ì²˜ë¦¬
        print(f"ğŸ”„ Converting {len(chunks)} chunks...")
        converted_chunks = []
        
        for i, chunk_data in enumerate(tqdm(chunks, desc="Converting")):
            chunk_start_time = time.time()
            
            if use_f0 and f0 is not None:
                converted_chunk = self.convert_single_chunk(
                    chunk_data['audio'],
                    target_speaker_id,
                    chunk_data['f0'],
                    chunk_data['vuv'],
                    method,
                    num_steps
                )
            else:
                converted_chunk = self.convert_single_chunk(
                    chunk_data['audio'],
                    target_speaker_id,
                    None,
                    None,
                    method,
                    num_steps
                )
            
            converted_chunks.append(converted_chunk)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            chunk_time = time.time() - chunk_start_time
            self.inference_times.append(chunk_time)
            
            if torch.cuda.is_available():
                memory_mb = torch.cuda.memory_allocated(self.device) / 1024 / 1024
                self.memory_usage.append(memory_mb)
        
        # ì²­í¬ ë³‘í•©
        print("ğŸ”— Merging chunks...")
        positions = [{'start': c.get('start', 0), 'end': c.get('end', 0), 
                     'actual_length': c.get('actual_length', chunk_length)} 
                    for c in chunks]
        
        merged_audio = self.merge_chunks_with_crossfade(converted_chunks, positions, overlap)
        
        # ìŠ¤í…Œë ˆì˜¤ ë³€í™˜
        if merged_audio.dim() == 1:
            merged_audio = merged_audio.unsqueeze(0).repeat(2, 1)
        
        # ì €ì¥
        print(f"ğŸ’¾ Saving to {output_path}")
        merged_audio_save = merged_audio.cpu().float()  # CPU + FP32ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        torchaudio.save(output_path, merged_audio_save, 44100)
        
        # í†µê³„ ê³„ì‚°
        total_time = time.time() - start_time
        duration = merged_audio.size(1) / 44100
        rtf = duration / total_time
        
        stats = {
            'duration': duration,
            'total_time': total_time,
            'rtf': rtf,
            'chunks': len(chunks),
            'method': method,
            'steps': num_steps,
            'f0_used': use_f0,
            'avg_chunk_time': sum(self.inference_times[-len(chunks):]) / len(chunks),
            'peak_memory_mb': max(self.memory_usage[-len(chunks):]) if self.memory_usage else 0
        }
        
        print(f"ğŸ‰ Conversion completed!")
        print(f"ğŸ“Š Stats:")
        print(f"   Duration: {duration:.2f}s")
        print(f"   Total time: {total_time:.2f}s")
        print(f"   RTF: {rtf:.2f}x")
        print(f"   Method: {method} ({num_steps} steps)")
        print(f"   F0 conditioning: {'âœ…' if use_f0 else 'âŒ'}")
        print(f"   Peak memory: {stats['peak_memory_mb']:.1f}MB")
        
        return stats
    
    def _split_audio_simple(self, waveform: torch.Tensor, 
                          chunk_length: int, overlap: int) -> List[dict]:
        """ê°„ë‹¨í•œ ì˜¤ë””ì˜¤ ë¶„í•  (F0 ì—†ì´)"""
        channels, total_length = waveform.shape
        step_size = chunk_length - overlap
        
        chunks = []
        start = 0
        
        while start < total_length:
            end = min(start + chunk_length, total_length)
            
            audio_chunk = waveform[:, start:end]
            
            # íŒ¨ë”©
            if audio_chunk.size(1) < chunk_length:
                pad_length = chunk_length - audio_chunk.size(1)
                audio_chunk = torch.cat([
                    audio_chunk,
                    torch.zeros(channels, pad_length, device=waveform.device, dtype=waveform.dtype)
                ], dim=1)
            
            chunks.append({
                'audio': audio_chunk,
                'start': start,
                'end': end,
                'actual_length': min(chunk_length, total_length - start)
            })
            
            start += step_size
            
            if end >= total_length:
                break
        
        return chunks
    
    def benchmark_performance(self, audio_path: str, target_speaker_id: int,
                            methods: List[str] = None, 
                            step_counts: List[int] = None) -> dict:
        """ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        if methods is None:
            methods = ['fast_rectified', 'heun', 'euler']
        if step_counts is None:
            step_counts = [4, 6, 8, 12]
        
        print(f"ğŸš€ Benchmarking performance...")
        
        results = {}
        
        for method in methods:
            for steps in step_counts:
                key = f"{method}_{steps}steps"
                print(f"\nğŸ”„ Testing {key}...")
                
                try:
                    stats = self.convert_full_audio(
                        audio_path=audio_path,
                        target_speaker_id=target_speaker_id,
                        output_path=f"benchmark_{key}.wav",
                        method=method,
                        num_steps=steps,
                        use_f0=True
                    )
                    
                    results[key] = {
                        'rtf': stats['rtf'],
                        'total_time': stats['total_time'],
                        'avg_chunk_time': stats['avg_chunk_time'],
                        'peak_memory_mb': stats['peak_memory_mb']
                    }
                    
                    print(f"âœ… {key}: RTF={stats['rtf']:.2f}x")
                    
                except Exception as e:
                    print(f"âŒ {key} failed: {e}")
                    results[key] = {'error': str(e)}
        
        return results

def main():
    parser = argparse.ArgumentParser(description='Optimized Voice Conversion Inference')
    parser.add_argument('--model', '-m', required=True, help='Model checkpoint path')
    parser.add_argument('--input', '-i', required=True, help='Input audio file')
    parser.add_argument('--output', '-o', required=True, help='Output audio file')
    parser.add_argument('--speaker', '-s', type=int, required=True, help='Target speaker ID')
    
    # ìµœì í™” ì„¤ì •
    parser.add_argument('--device', default='auto', help='Device (auto/cuda/cpu)')
    parser.add_argument('--no-amp', action='store_true', help='Disable AMP FP16')
    parser.add_argument('--no-compile', action='store_true', help='Disable model compilation')
    parser.add_argument('--no-f0', action='store_true', help='Disable F0 conditioning')
    
    # ì¶”ë¡  ì„¤ì •
    parser.add_argument('--method', default='fast_rectified',
                       choices=['fast_rectified', 'heun', 'rk4', 'euler'],
                       help='Inference method')
    parser.add_argument('--steps', type=int, default=6, help='Number of inference steps')
    parser.add_argument('--chunk-length', type=int, default=16384, help='Chunk length')
    parser.add_argument('--overlap', type=int, default=2048, help='Chunk overlap')
    
    # ë²¤ì¹˜ë§ˆí¬
    parser.add_argument('--benchmark', action='store_true', help='Run performance benchmark')
    
    args = parser.parse_args()
    
    # ê²½ë¡œ ê²€ì¦
    model_path = Path(args.model)
    input_path = Path(args.input)
    output_path = Path(args.output)
    
    if not model_path.exists():
        print(f"âŒ Model not found: {model_path}")
        return
    
    if not input_path.exists():
        print(f"âŒ Input not found: {input_path}")
        return
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    print("ğŸš€ Starting Optimized Voice Conversion")
    print(f"   Model: {model_path}")
    print(f"   Input: {input_path}")
    print(f"   Output: {output_path}")
    print(f"   Speaker: {args.speaker}")
    print(f"   Method: {args.method} ({args.steps} steps)")
    print(f"   F0 conditioning: {'âœ…' if not args.no_f0 else 'âŒ'}")
    
    # ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
    engine = OptimizedInferenceEngine(
        model_path=str(model_path),
        device=args.device,
        use_amp=not args.no_amp,
        compile_model=not args.no_compile
    )
    
    if args.benchmark:
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        results = engine.benchmark_performance(
            audio_path=str(input_path),
            target_speaker_id=args.speaker
        )
        
        print("\nğŸ“Š Benchmark Results:")
        for key, stats in results.items():
            if 'error' not in stats:
                print(f"   {key}: RTF={stats['rtf']:.2f}x, Memory={stats['peak_memory_mb']:.1f}MB")
    else:
        # ì¼ë°˜ ë³€í™˜
        stats = engine.convert_full_audio(
            audio_path=str(input_path),
            target_speaker_id=args.speaker,
            output_path=str(output_path),
            chunk_length=args.chunk_length,
            overlap=args.overlap,
            method=args.method,
            num_steps=args.steps,
            use_f0=not args.no_f0
        )

if __name__ == '__main__':
    main()

"""
ğŸš€ Optimized Inference Examples:

# 1. ê¸°ë³¸ ë¹ ë¥¸ ë³€í™˜
python inference.py -m model.pt -i input.wav -o output.wav -s 1

# 2. ê³ í’ˆì§ˆ ë³€í™˜ (ë” ë§ì€ ë‹¨ê³„)
python inference.py -m model.pt -i input.wav -o output.wav -s 1 \\
                   --method heun --steps 12

# 3. ì´ˆê³ ì† ë³€í™˜ (F0 ì—†ì´)
python inference.py -m model.pt -i input.wav -o output.wav -s 1 \\
                   --method fast_rectified --steps 4 --no-f0

# 4. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
python inference.py -m model.pt -i input.wav -o output.wav -s 1 --benchmark

# 5. CPU ì¶”ë¡ 
python inference.py -m model.pt -i input.wav -o output.wav -s 1 \\
                   --device cpu --no-amp --no-compile

# 6. ë©”ëª¨ë¦¬ ì ˆì•½ (ì‘ì€ ì²­í¬)
python inference.py -m model.pt -i input.wav -o output.wav -s 1 \\
                   --chunk-length 8192 --overlap 1024
"""